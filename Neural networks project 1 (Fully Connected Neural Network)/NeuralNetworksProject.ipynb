{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "XH1xUBcGjAru",
    "outputId": "86757617-4080-47b0-a6b2-0d0d7e030dc8"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'drive' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1ea353ebe4c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'drive' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We upload the datset in google drive and mount google drive on the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y3-diFut79T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Read the data from the h5py file and understand the train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S-OYqBpM1djm",
    "outputId": "baa7b71c-3c2c-4427-acc9-b6e157542b12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "h= h5py.File('/content/drive/My Drive/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "h.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMhxCZU2rTO_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOLLLnO00Jx4"
   },
   "outputs": [],
   "source": [
    "X_train = h['X_train'][:]\n",
    "y_train = h['y_train'][:]\n",
    "y_test= h['y_test'][:]\n",
    "X_test = h['X_test'][:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We visualise train data and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "dvpzUnlduSmB",
    "outputId": "6eb1d386-770e-4c41-e1b0-dd7204b2d505"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  6\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[1000])    \n",
    "plt.show()\n",
    "print('Label: ', y_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "zYaikXTD41jr",
    "outputId": "2331d69f-3cd7-41ee-9ebb-6cc68f90ae4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 32, 32)\n",
      "(42000,)\n",
      "(18000,)\n",
      "(18000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecevkMr_435y"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Reshape and normalize the train and test features "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We flatten X_train and X_test Data from 2d(32*32) to 1d(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NqWZVn_XsLqi",
    "outputId": "6e465f53-0ef0-4fb1-b261-3100c7d87521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024)\n",
      "(18000, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(42000, 1024)\n",
    "print(X_train.shape)\n",
    "X_test = X_test.reshape(18000,1024)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "kAHUzmgqWRAI",
    "outputId": "5f5a50ed-b3fb-4ecf-dea8-11188386f896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.9745\n",
      "0.0\n",
      "0.9999\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) One hot encode the labels for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAwCixVa5yMb"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "\n",
    "y_test = to_categorical(y_test, num_classes= 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vbB9ZGSiyfSH",
    "outputId": "8267ea05-ef7d-40e8-a536-6f901f5c72e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 10) (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cpQ2PXPy0sP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Define the model architecture using TensorFlow with a flatten layer followed by dense layers with activation as ReLu and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrBKcNSTG88L"
   },
   "source": [
    "MODEL_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9xRh_FS7swM"
   },
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(50, input_shape = (1024,), activation= \"relu\"))\n",
    "model.add(Dense(50, activation= \"relu\"))\n",
    "model.add(Dense(50, activation= \"sigmoid\"))\n",
    "model.add(Dense(10, activation= \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)Compile the model with loss as categorical cross-entropy and adam optimizers. Use accuracy as the metric for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6RWAO9pHpiQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "drtz6N27_WyT",
    "outputId": "8e5ee7ab-1667-4059-c588-caf5f829f04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.4535 - accuracy: 0.1026\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.3668 - accuracy: 0.1040\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.3436 - accuracy: 0.1034\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3307 - accuracy: 0.1046\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3222 - accuracy: 0.1065\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3160 - accuracy: 0.1081\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3112 - accuracy: 0.1080\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3073 - accuracy: 0.1140\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.3040 - accuracy: 0.1180\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.3010 - accuracy: 0.1198\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2987 - accuracy: 0.1221\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2963 - accuracy: 0.1245\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2943 - accuracy: 0.1266\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 2.2922 - accuracy: 0.1285\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2902 - accuracy: 0.1303\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2885 - accuracy: 0.1331\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2869 - accuracy: 0.1341\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2854 - accuracy: 0.1367\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2836 - accuracy: 0.1398\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2821 - accuracy: 0.1417\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2807 - accuracy: 0.1425\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 2.2790 - accuracy: 0.1443\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2772 - accuracy: 0.1460\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2757 - accuracy: 0.1475\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2737 - accuracy: 0.1497\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2719 - accuracy: 0.1505\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2701 - accuracy: 0.1527\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2679 - accuracy: 0.1559\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2663 - accuracy: 0.1569\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2644 - accuracy: 0.1595\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2627 - accuracy: 0.1621\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2607 - accuracy: 0.1650\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2588 - accuracy: 0.1676\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2569 - accuracy: 0.1717\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 2.2549 - accuracy: 0.1730\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2531 - accuracy: 0.1759\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2510 - accuracy: 0.1790\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2487 - accuracy: 0.1819\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2465 - accuracy: 0.1860\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2444 - accuracy: 0.1873\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2423 - accuracy: 0.1885\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2404 - accuracy: 0.1909\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2386 - accuracy: 0.1931\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2366 - accuracy: 0.1954\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2347 - accuracy: 0.1970\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2326 - accuracy: 0.1998\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2305 - accuracy: 0.2005\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2285 - accuracy: 0.2012\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2263 - accuracy: 0.2064\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2243 - accuracy: 0.2080\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2220 - accuracy: 0.2118\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2201 - accuracy: 0.2130\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2180 - accuracy: 0.2158\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2159 - accuracy: 0.2178\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2139 - accuracy: 0.2192\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2119 - accuracy: 0.2210\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2101 - accuracy: 0.2236\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2081 - accuracy: 0.2254\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2064 - accuracy: 0.2257\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2047 - accuracy: 0.2263\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2029 - accuracy: 0.2278\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2010 - accuracy: 0.2307\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.1994 - accuracy: 0.2291\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1978 - accuracy: 0.2302\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1962 - accuracy: 0.2316\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1944 - accuracy: 0.2329\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1929 - accuracy: 0.2312\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1910 - accuracy: 0.2289\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1895 - accuracy: 0.2247\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.1880 - accuracy: 0.2256\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1863 - accuracy: 0.2251\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1847 - accuracy: 0.2276\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1832 - accuracy: 0.2283\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1815 - accuracy: 0.2274\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1798 - accuracy: 0.2279\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1783 - accuracy: 0.2298\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1766 - accuracy: 0.2307\n",
      "Epoch 78/100\n",
      "18/42 [===========>..................] - ETA: 0s - loss: 2.1727 - accuracy: 0.2358"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8f64dd7d4e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model.compile(optimizer= sgd, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size = 1000, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)Fit and evaluate the model. Print the loss and accuracy for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "MaW3DInNBvWQ",
    "outputId": "0af895af-5a3b-4acf-de89-edc88aab8259"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-6e2cb98d3b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test accuracy :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2567\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2569\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2570\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "res_1 = model.evaluate(X_test, y_test)\n",
    "print(\"test accuracy :\", res_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gvN47krbthG-",
    "outputId": "4054d745-950b-4bb6-c739-209e3e598628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 56,860\n",
      "Trainable params: 56,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUfIKAo5Gks2"
   },
   "source": [
    "The test accuracy is not good for this model. Its around 22.6% which is very low. we use adam optimer instead of sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxpbCYI6HL5i"
   },
   "source": [
    "Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAZiDwfsGaqc"
   },
   "outputs": [],
   "source": [
    "model_2 = Sequential() \n",
    "model_2.add(Dense(50, input_shape = (1024,), activation= \"relu\"))\n",
    "model_2.add(Dense(50, activation= \"relu\"))\n",
    "model_2.add(Dense(50, activation= \"sigmoid\"))\n",
    "model_2.add(Dense(10, activation= \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "snEgMm5DtwAK",
    "outputId": "34eeb553-5ce2-40f8-a69a-eaa03d43ae0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 56,860\n",
      "Trainable params: 56,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MrKQz50ZH9S0",
    "outputId": "0392b1f4-9d6c-4351-a1a5-ecc8b8290ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 2.3188 - accuracy: 0.0988\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.3018 - accuracy: 0.1075\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2847 - accuracy: 0.1409\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2216 - accuracy: 0.1953\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.0883 - accuracy: 0.2613\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.9430 - accuracy: 0.3217\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.8265 - accuracy: 0.3617\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.7479 - accuracy: 0.3961\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6984 - accuracy: 0.4111\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6668 - accuracy: 0.4222\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6294 - accuracy: 0.4388\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.6011 - accuracy: 0.4492\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5873 - accuracy: 0.4497\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5629 - accuracy: 0.4637\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5467 - accuracy: 0.4703\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5218 - accuracy: 0.4829\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5000 - accuracy: 0.4947\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.4696 - accuracy: 0.5089\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4388 - accuracy: 0.5250\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.4040 - accuracy: 0.5380\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3714 - accuracy: 0.5542\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3481 - accuracy: 0.5623\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3245 - accuracy: 0.5727\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.2951 - accuracy: 0.5838\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.2754 - accuracy: 0.5891\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.2560 - accuracy: 0.5957\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2416 - accuracy: 0.6005\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2271 - accuracy: 0.6037\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2119 - accuracy: 0.6098\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2023 - accuracy: 0.6121\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1936 - accuracy: 0.6167\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1799 - accuracy: 0.6195\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1717 - accuracy: 0.6236\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1688 - accuracy: 0.6244\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1620 - accuracy: 0.6245\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1538 - accuracy: 0.6281\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1372 - accuracy: 0.6345\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1342 - accuracy: 0.6361\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1209 - accuracy: 0.6410\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1173 - accuracy: 0.6428\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1102 - accuracy: 0.6436\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.1059 - accuracy: 0.6467\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0945 - accuracy: 0.6485\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0872 - accuracy: 0.6530\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0844 - accuracy: 0.6543\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0846 - accuracy: 0.6525\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.0742 - accuracy: 0.6540\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0693 - accuracy: 0.6578\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0602 - accuracy: 0.6613\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0580 - accuracy: 0.6618\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0458 - accuracy: 0.6661\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0447 - accuracy: 0.6674\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0333 - accuracy: 0.6717\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0316 - accuracy: 0.6712\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0266 - accuracy: 0.6731\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0190 - accuracy: 0.6756\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0158 - accuracy: 0.6773\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0143 - accuracy: 0.6781\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0136 - accuracy: 0.6774\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.0045 - accuracy: 0.6815\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9983 - accuracy: 0.6844\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.9951 - accuracy: 0.6840\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9935 - accuracy: 0.6861\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9852 - accuracy: 0.6883\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9814 - accuracy: 0.6894\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9797 - accuracy: 0.6901\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.9763 - accuracy: 0.6915\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9681 - accuracy: 0.6952\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9622 - accuracy: 0.6964\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9596 - accuracy: 0.6976\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9576 - accuracy: 0.6969\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9544 - accuracy: 0.6995\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9594 - accuracy: 0.6989\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9478 - accuracy: 0.7008\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9406 - accuracy: 0.7025\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9384 - accuracy: 0.7030\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9312 - accuracy: 0.7070\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9289 - accuracy: 0.7076\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9320 - accuracy: 0.7054\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9282 - accuracy: 0.7071\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9195 - accuracy: 0.7103\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9146 - accuracy: 0.7125\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9117 - accuracy: 0.7140\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9221 - accuracy: 0.7097\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.9141 - accuracy: 0.7128\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.9050 - accuracy: 0.7151\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.8987 - accuracy: 0.7186\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.8961 - accuracy: 0.7172\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.8945 - accuracy: 0.7198\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8932 - accuracy: 0.7192\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8865 - accuracy: 0.7223\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8864 - accuracy: 0.7223\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8815 - accuracy: 0.7233\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.8761 - accuracy: 0.7256\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8763 - accuracy: 0.7250\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8756 - accuracy: 0.7264\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8859 - accuracy: 0.7224\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8736 - accuracy: 0.7244\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8635 - accuracy: 0.7292\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.8661 - accuracy: 0.7270\n"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model_2.compile(optimizer= adam, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "history_2 = model_2.fit(X_train, y_train, batch_size = 1000, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9H9BxXtgIsop",
    "outputId": "194ae79f-968e-4674-e653-edf9a390c58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 1ms/step - loss: 0.9079 - accuracy: 0.7171\n",
      "0.7170555591583252\n"
     ]
    }
   ],
   "source": [
    "res_2 = model_2.evaluate(X_test, y_test)\n",
    "print(res_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "4LXYtVt9ZwwA",
    "outputId": "1f1e3f28-1234-4bc8-bbbf-d78a8a4dd954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [6.5596873e-04 6.7008077e-03 4.1508052e-04 2.1175304e-04 9.6479809e-01\n",
      " 5.0261401e-04 2.1965127e-02 3.4394537e-04 3.9944826e-03 4.1217785e-04]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = model_2.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9ppqkp-LidI"
   },
   "source": [
    "This model is far better than the previous model . The test accuracy is around 70% which is average performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9yChnYDa6SY"
   },
   "source": [
    "**`MODEL_3`**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we do some regularisation in this model. We add a Batch Normalisation layer which normalises data entering every hidden layer in each epoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7lLaVj_Lezw"
   },
   "outputs": [],
   "source": [
    "def model_3():\n",
    "  model_3 = Sequential() \n",
    "  model_3.add(Dense(50, input_shape = (1024,), activation= \"relu\"))\n",
    "  model_3.add(BatchNormalization())\n",
    "  model_3.add(Dense(50, activation= \"relu\"))\n",
    "  model_3.add(BatchNormalization())\n",
    "  model_3.add(Dense(50, activation= \"sigmoid\"))\n",
    "  model_3.add(BatchNormalization())\n",
    "  model_3.add(Dense(10, activation= \"softmax\"))\n",
    "  ada = optimizers.Adam(lr = 0.001)\n",
    "  model_3.compile(optimizer = ada, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "  return model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nz13XkSHbuUa"
   },
   "source": [
    "In this model we do normalise the inputs for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "F4wWDMcLbtX1",
    "outputId": "6a063659-c313-4943-b824-30199b6601ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 57,460\n",
      "Trainable params: 57,160\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3= model_3()\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMYBUtdPt_Bg"
   },
   "outputs": [],
   "source": [
    "\n",
    "history_3 = model_3.fit(X_train, y_train, epochs = 100, batch_size = 1000, verbose = 1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "lztGgnEHNSKa",
    "outputId": "5f3060ab-f530-41c1-af31-91838b108951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 1.0031 - accuracy: 0.7266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7265555262565613"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_3 = model_3.evaluate(X_test, y_test)\n",
    "res_3[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "8qyZg170kbG-",
    "outputId": "72df854f-9048-40a5-da90-5c7666de7275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [6.1194937e-06 1.0057158e-03 6.1379515e-06 5.9949001e-04 9.9252921e-01\n",
      " 1.9068085e-03 2.3469067e-05 1.8097789e-07 3.9225519e-03 3.1359738e-07]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = model_3.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqDIe3UnkK_A"
   },
   "source": [
    "The test acccuracy is average but better than model_2. We are trying to reduce overfitting it by Adding dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7DIUHQfl30h"
   },
   "outputs": [],
   "source": [
    "def model_4():\n",
    "  model_4 = Sequential() \n",
    "  model_4.add(Dense(50, input_shape = (1024,), activation= \"relu\"))\n",
    "  model_4.add(BatchNormalization())\n",
    "  model_4.add(Dropout(0.1))\n",
    "  model_4.add(Dense(50, activation= \"relu\"))\n",
    "  model_4.add(BatchNormalization())\n",
    "  model_4.add(Dropout(0.1))\n",
    "  \n",
    "  model_4.add(Dense(50, activation= \"sigmoid\"))\n",
    "  model_4.add(BatchNormalization())\n",
    "  model_4.add(Dropout(0.1))\n",
    "  model_4.add(Dense(10, activation= \"softmax\"))\n",
    "  ada = optimizers.Adam(lr = 0.001)\n",
    "  model_4.compile(optimizer = ada, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "  return model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "1q4q5POqmghr",
    "outputId": "444ea33d-d4f1-4dd0-8acd-a31a6c05dd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 57,460\n",
      "Trainable params: 57,160\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a = model_4()\n",
    "a.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEZEEC2KuSbo"
   },
   "outputs": [],
   "source": [
    "history_4 = a.fit(X_train, y_train, epochs = 100, batch_size = 1000, verbose = 1, validation_data= (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "7UIZoUmqnTOj",
    "outputId": "a27b974e-39d6-46c9-b84d-c9a72001f51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6894 - accuracy: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7919999957084656"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  res_4 = a.evaluate(X_test, y_test)\n",
    "res_4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "aZJ6LU-Uguec",
    "outputId": "400b2682-fd35-4ecd-c929-dee97ed87908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [2.6649007e-04 4.2005875e-03 1.6370307e-04 9.2357637e-05 9.9126500e-01\n",
      " 8.4976156e-05 1.4935805e-03 2.2058159e-05 2.2312354e-03 1.7993175e-04]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = a.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcpfOykuuFHl"
   },
   "source": [
    "The test accuracy for model_4 has improved. Lets try to improve it ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7N4GnsBkggXj"
   },
   "source": [
    "MODEL 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfofgqUYgk-M"
   },
   "source": [
    "Lets try to do some weight initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJpRHv1CuGfA"
   },
   "outputs": [],
   "source": [
    "def model_5():\n",
    "  model_5 = Sequential() \n",
    "  model_5.add(Dense(50, input_shape = (1024,), activation= \"relu\", kernel_initializer='he_normal'))\n",
    "  model_5.add(BatchNormalization())\n",
    "  model_5.add(Dropout(0.1))\n",
    "  model_5.add(Dense(50, activation= \"relu\", kernel_initializer='he_normal'))\n",
    "  model_5.add(BatchNormalization())\n",
    "  model_5.add(Dropout(0.1))\n",
    "  \n",
    "  model_5.add(Dense(50, activation= \"sigmoid\", kernel_initializer='he_normal'))\n",
    "  model_5.add(BatchNormalization())\n",
    "  model_5.add(Dropout(0.1))\n",
    "  model_5.add(Dense(10, activation= \"softmax\", kernel_initializer='he_normal'))\n",
    "  ada = optimizers.Adam(lr = 0.001)\n",
    "  model_5.compile(optimizer = ada, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "  return model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "U9FSEBp2es2i",
    "outputId": "0ffceee4-d34c-4fd8-f660-3e801f18bd0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 50)                51250     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 57,460\n",
      "Trainable params: 57,160\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "b = model_5()\n",
    "b.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSgfC4h4ueto"
   },
   "outputs": [],
   "source": [
    "history_5 = b.fit(X_train, y_train, epochs = 100, batch_size = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "d2DvkhxmfbtS",
    "outputId": "a896972b-aedb-4897-b771-d09ca62beb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 1ms/step - loss: 0.7577 - accuracy: 0.7680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7680000066757202"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_5 = b.evaluate(X_test, y_test)\n",
    "res_5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "WvHCSDXYgg87",
    "outputId": "59b10e86-1d75-4e05-b886-51f0f4d2cd87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [3.0544717e-05 3.7399479e-04 2.8974673e-05 2.9510580e-05 9.9818355e-01\n",
      " 5.8094789e-05 2.6494052e-04 1.4598330e-06 9.8735478e-04 4.1593419e-05]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = b.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2TbpLDDgpks"
   },
   "source": [
    "The test accuracy has improve but we need improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty3dbl91hDr7"
   },
   "source": [
    "en l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2LXPbOcg-iW"
   },
   "source": [
    "MODEL 6\n",
    "\n",
    "Lets try to change the no of hidden layers and the no of neurons in every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dl-jPaDAgo4C"
   },
   "outputs": [],
   "source": [
    "def model_6():\n",
    "  model_6 = Sequential() \n",
    "  model_6.add(Dense(256, input_shape = (1024,)))\n",
    "  model_6.add(Activation('relu'))\n",
    "  model_6.add(BatchNormalization())\n",
    "  \n",
    "  model_6.add(Dense(256))\n",
    "  model_6.add(BatchNormalization())\n",
    "  model_6.add(Activation('relu'))\n",
    "\n",
    "  model_6.add(Dense(10))\n",
    "  model_6.add(Activation('softmax'))\n",
    "\n",
    "  ada_2 = optimizers.Adam(lr = 0.0002977345377574985)\n",
    "  model_6.compile(optimizer=ada_2, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "\n",
    "  return model_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "0I2dBdtRgNoi",
    "outputId": "3e2eeade-c2aa-4a41-bf65-61a1c514db1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 332,810\n",
      "Trainable params: 331,786\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = model_6()\n",
    "model_6.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfhZvw_-uwQR"
   },
   "outputs": [],
   "source": [
    "history_6 = model_6.fit(X_train, y_train, batch_size = 1000, epochs= 100, verbose = 1, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kMjVEG_pKP3s",
    "outputId": "439ac704-9fe8-4312-8850-3e65440a0821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.8152 - accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7925000190734863"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_6 = model_6.evaluate(X_test, y_test)\n",
    "res_6[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "WjjTgaIuga71",
    "outputId": "e41cc275-2e42-4c93-d5e1-dcbcaa2279dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [2.31040900e-07 1.83310058e-05 1.40111985e-11 1.30758588e-06\n",
      " 9.99600470e-01 2.27546479e-06 3.76944576e-04 2.20263829e-09\n",
      " 5.10328334e-07 5.27477217e-09]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = model_6.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2e6ivuVfM5-Q"
   },
   "source": [
    "We see that for this model, the test accuracy has increased a little but model is overfitting. we do some regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePXxCEvjOGkZ"
   },
   "source": [
    "***MODEL_7***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We pass different values of  beta 1, beta 2 and learning rate to evaluate this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j34TrksPM1NT"
   },
   "outputs": [],
   "source": [
    "def model_7(beta1, beta2, lr):\n",
    "  Beta1 =beta1\n",
    "  Beta2= beta2\n",
    "  learning_rate= lr\n",
    "  model_7 = Sequential() \n",
    "  model_7.add(Dense(256, input_shape = (1024,)))\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Dropout(0.1))\n",
    "  \n",
    "  model_7.add(Dense(256))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(Dropout(0.1))\n",
    "\n",
    "  model_7.add(Dense(256))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "  model_7.add(Dense(10))\n",
    "  model_7.add(Activation('softmax'))\n",
    "\n",
    "  ada_3 = optimizers.Adam(lr = learning_rate, beta_1= Beta1, beta_2 = Beta2, epsilon=1e-07)\n",
    "  model_7.compile(optimizer=ada_3, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "  #callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.01)\n",
    "  model_7.fit(X_train, y_train, batch_size = 1000, epochs= 100, verbose = 1, validation_data=(X_test,y_test)) \n",
    "  print(\"summary of model\" , model_7.summary())\n",
    "  RES_7 = model_7.evaluate(X_test, y_test)\n",
    "\n",
    "  return model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nDsy4UKpN-dj",
    "outputId": "ecaa78a0-f98f-4e55-f80d-84fdfd1f2e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 1.8333 - accuracy: 0.3762 - val_loss: 2.0293 - val_accuracy: 0.4512\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 1.1017 - accuracy: 0.6577 - val_loss: 1.6401 - val_accuracy: 0.5782\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.9320 - accuracy: 0.7119 - val_loss: 1.4808 - val_accuracy: 0.5977\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.8472 - accuracy: 0.7385 - val_loss: 1.3460 - val_accuracy: 0.6339\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.7868 - accuracy: 0.7582 - val_loss: 1.2079 - val_accuracy: 0.6337\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.7288 - accuracy: 0.7745 - val_loss: 1.1700 - val_accuracy: 0.6238\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.6874 - accuracy: 0.7881 - val_loss: 1.1508 - val_accuracy: 0.6064\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6506 - accuracy: 0.7993 - val_loss: 1.0778 - val_accuracy: 0.6351\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6259 - accuracy: 0.8061 - val_loss: 1.0956 - val_accuracy: 0.6514\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.6026 - accuracy: 0.8145 - val_loss: 1.0032 - val_accuracy: 0.6826\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5812 - accuracy: 0.8196 - val_loss: 1.0901 - val_accuracy: 0.6432\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5590 - accuracy: 0.8262 - val_loss: 0.7853 - val_accuracy: 0.7579\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5463 - accuracy: 0.8303 - val_loss: 0.7741 - val_accuracy: 0.7567\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5298 - accuracy: 0.8348 - val_loss: 0.8350 - val_accuracy: 0.7414\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5143 - accuracy: 0.8407 - val_loss: 0.6898 - val_accuracy: 0.7846\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4993 - accuracy: 0.8442 - val_loss: 0.8113 - val_accuracy: 0.7489\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4998 - accuracy: 0.8435 - val_loss: 0.6744 - val_accuracy: 0.7928\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4879 - accuracy: 0.8468 - val_loss: 0.7003 - val_accuracy: 0.7847\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4738 - accuracy: 0.8512 - val_loss: 0.6878 - val_accuracy: 0.7883\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4723 - accuracy: 0.8513 - val_loss: 0.9167 - val_accuracy: 0.7138\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4544 - accuracy: 0.8577 - val_loss: 1.0346 - val_accuracy: 0.6804\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4461 - accuracy: 0.8609 - val_loss: 0.5840 - val_accuracy: 0.8244\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4468 - accuracy: 0.8592 - val_loss: 0.6863 - val_accuracy: 0.7944\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4373 - accuracy: 0.8623 - val_loss: 0.7529 - val_accuracy: 0.7697\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4354 - accuracy: 0.8632 - val_loss: 0.6246 - val_accuracy: 0.8076\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4246 - accuracy: 0.8644 - val_loss: 0.6915 - val_accuracy: 0.7954\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4193 - accuracy: 0.8669 - val_loss: 0.6132 - val_accuracy: 0.8122\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4134 - accuracy: 0.8676 - val_loss: 0.5922 - val_accuracy: 0.8173\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4016 - accuracy: 0.8734 - val_loss: 0.5887 - val_accuracy: 0.8213\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4033 - accuracy: 0.8703 - val_loss: 0.5457 - val_accuracy: 0.8372\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4005 - accuracy: 0.8726 - val_loss: 0.6611 - val_accuracy: 0.7978\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3992 - accuracy: 0.8721 - val_loss: 0.5452 - val_accuracy: 0.8348\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3906 - accuracy: 0.8750 - val_loss: 0.6480 - val_accuracy: 0.8034\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3874 - accuracy: 0.8760 - val_loss: 0.6056 - val_accuracy: 0.8106\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3756 - accuracy: 0.8794 - val_loss: 0.5358 - val_accuracy: 0.8396\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3767 - accuracy: 0.8791 - val_loss: 0.5542 - val_accuracy: 0.8367\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.3665 - accuracy: 0.8841 - val_loss: 0.6445 - val_accuracy: 0.8075\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3630 - accuracy: 0.8824 - val_loss: 0.6949 - val_accuracy: 0.7853\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3628 - accuracy: 0.8831 - val_loss: 0.6321 - val_accuracy: 0.8058\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3624 - accuracy: 0.8836 - val_loss: 0.6652 - val_accuracy: 0.7962\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3671 - accuracy: 0.8824 - val_loss: 0.6183 - val_accuracy: 0.8129\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3539 - accuracy: 0.8850 - val_loss: 0.6119 - val_accuracy: 0.8138\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3492 - accuracy: 0.8865 - val_loss: 0.5453 - val_accuracy: 0.8345\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3369 - accuracy: 0.8912 - val_loss: 0.6010 - val_accuracy: 0.8207\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3493 - accuracy: 0.8861 - val_loss: 0.7362 - val_accuracy: 0.7842\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3480 - accuracy: 0.8875 - val_loss: 0.6209 - val_accuracy: 0.8147\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3409 - accuracy: 0.8915 - val_loss: 0.6029 - val_accuracy: 0.8226\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3371 - accuracy: 0.8920 - val_loss: 0.5400 - val_accuracy: 0.8408\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3262 - accuracy: 0.8942 - val_loss: 0.5929 - val_accuracy: 0.8279\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3223 - accuracy: 0.8946 - val_loss: 0.5009 - val_accuracy: 0.8529\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3260 - accuracy: 0.8935 - val_loss: 0.5596 - val_accuracy: 0.8306\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3198 - accuracy: 0.8951 - val_loss: 0.5966 - val_accuracy: 0.8242\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3271 - accuracy: 0.8940 - val_loss: 0.6682 - val_accuracy: 0.7970\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3163 - accuracy: 0.8975 - val_loss: 0.6246 - val_accuracy: 0.8146\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3199 - accuracy: 0.8963 - val_loss: 0.6064 - val_accuracy: 0.8229\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3098 - accuracy: 0.8989 - val_loss: 0.5889 - val_accuracy: 0.8285\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3083 - accuracy: 0.8998 - val_loss: 0.5588 - val_accuracy: 0.8337\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3094 - accuracy: 0.8985 - val_loss: 0.5481 - val_accuracy: 0.8416\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3104 - accuracy: 0.8981 - val_loss: 0.5490 - val_accuracy: 0.8383\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2916 - accuracy: 0.9045 - val_loss: 0.5100 - val_accuracy: 0.8486\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2894 - accuracy: 0.9040 - val_loss: 0.5589 - val_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2958 - accuracy: 0.9026 - val_loss: 0.5464 - val_accuracy: 0.8419\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2947 - accuracy: 0.9021 - val_loss: 0.5231 - val_accuracy: 0.8478\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2844 - accuracy: 0.9068 - val_loss: 0.5438 - val_accuracy: 0.8428\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2864 - accuracy: 0.9043 - val_loss: 0.5958 - val_accuracy: 0.8296\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2912 - accuracy: 0.9044 - val_loss: 0.5428 - val_accuracy: 0.8423\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2883 - accuracy: 0.9055 - val_loss: 0.5566 - val_accuracy: 0.8374\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2832 - accuracy: 0.9068 - val_loss: 0.5639 - val_accuracy: 0.8456\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.2797 - accuracy: 0.9077 - val_loss: 0.6284 - val_accuracy: 0.8186\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2790 - accuracy: 0.9083 - val_loss: 0.6293 - val_accuracy: 0.8252\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2710 - accuracy: 0.9101 - val_loss: 0.5805 - val_accuracy: 0.8309\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2749 - accuracy: 0.9102 - val_loss: 0.5740 - val_accuracy: 0.8311\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2687 - accuracy: 0.9121 - val_loss: 0.5711 - val_accuracy: 0.8326\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2704 - accuracy: 0.9100 - val_loss: 0.5312 - val_accuracy: 0.8482\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2657 - accuracy: 0.9112 - val_loss: 0.6014 - val_accuracy: 0.8301\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.2710 - accuracy: 0.9104 - val_loss: 0.5470 - val_accuracy: 0.8434\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2592 - accuracy: 0.9162 - val_loss: 0.5018 - val_accuracy: 0.8597\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2650 - accuracy: 0.9135 - val_loss: 0.5217 - val_accuracy: 0.8510\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2632 - accuracy: 0.9125 - val_loss: 0.6356 - val_accuracy: 0.8159\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2622 - accuracy: 0.9134 - val_loss: 0.5488 - val_accuracy: 0.8419\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2502 - accuracy: 0.9172 - val_loss: 0.5057 - val_accuracy: 0.8574\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2572 - accuracy: 0.9153 - val_loss: 0.6116 - val_accuracy: 0.8228\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2481 - accuracy: 0.9180 - val_loss: 0.5219 - val_accuracy: 0.8521\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2505 - accuracy: 0.9187 - val_loss: 0.6004 - val_accuracy: 0.8352\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2480 - accuracy: 0.9179 - val_loss: 0.5008 - val_accuracy: 0.8634\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2427 - accuracy: 0.9193 - val_loss: 0.5205 - val_accuracy: 0.8527\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2449 - accuracy: 0.9187 - val_loss: 0.5632 - val_accuracy: 0.8378\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2476 - accuracy: 0.9187 - val_loss: 0.5394 - val_accuracy: 0.8498\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2482 - accuracy: 0.9184 - val_loss: 0.5836 - val_accuracy: 0.8352\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2444 - accuracy: 0.9197 - val_loss: 0.5252 - val_accuracy: 0.8568\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2436 - accuracy: 0.9193 - val_loss: 0.5240 - val_accuracy: 0.8588\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2385 - accuracy: 0.9215 - val_loss: 0.4992 - val_accuracy: 0.8631\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.2401 - accuracy: 0.9205 - val_loss: 0.5736 - val_accuracy: 0.8363\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2418 - accuracy: 0.9194 - val_loss: 0.6207 - val_accuracy: 0.8286\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2438 - accuracy: 0.9189 - val_loss: 0.5435 - val_accuracy: 0.8488\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.2397 - accuracy: 0.9217 - val_loss: 0.6034 - val_accuracy: 0.8394\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2399 - accuracy: 0.9207 - val_loss: 0.5304 - val_accuracy: 0.8552\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2353 - accuracy: 0.9214 - val_loss: 0.6059 - val_accuracy: 0.8330\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.2267 - accuracy: 0.9254 - val_loss: 0.5243 - val_accuracy: 0.8537\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.2332 - accuracy: 0.9215 - val_loss: 0.6667 - val_accuracy: 0.8132\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6667 - accuracy: 0.8132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f51c88ae5f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7(0.9, 0.99, 0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extremely less test accuracy for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uL37y9i6N-rV",
    "outputId": "0ef9c930-b28a-4352-81c1-7967b491a687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 3472424.5000 - accuracy: 0.1013 - val_loss: 1086332800.0000 - val_accuracy: 0.0972\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 789.8948 - accuracy: 0.0980 - val_loss: 111635312.0000 - val_accuracy: 0.0998\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 643.9875 - accuracy: 0.0983 - val_loss: 84264768.0000 - val_accuracy: 0.1021\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 582.9138 - accuracy: 0.1002 - val_loss: 81857688.0000 - val_accuracy: 0.1014\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 534.3077 - accuracy: 0.1001 - val_loss: 90358344.0000 - val_accuracy: 0.1007\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 572.1898 - accuracy: 0.0988 - val_loss: 104139256.0000 - val_accuracy: 0.1002\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 623.3505 - accuracy: 0.0994 - val_loss: 121517240.0000 - val_accuracy: 0.1017\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 569.7046 - accuracy: 0.1027 - val_loss: 143123888.0000 - val_accuracy: 0.1007\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 543.3117 - accuracy: 0.1003 - val_loss: 166608160.0000 - val_accuracy: 0.1006\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 607.9388 - accuracy: 0.1002 - val_loss: 200526640.0000 - val_accuracy: 0.1002\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 592.4066 - accuracy: 0.1018 - val_loss: 241424688.0000 - val_accuracy: 0.1017\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 601.1686 - accuracy: 0.0992 - val_loss: 285083392.0000 - val_accuracy: 0.0955\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 555.0480 - accuracy: 0.1007 - val_loss: 332052832.0000 - val_accuracy: 0.1017\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 647.1288 - accuracy: 0.0978 - val_loss: 389423680.0000 - val_accuracy: 0.0955\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 687.2125 - accuracy: 0.1026 - val_loss: 473840128.0000 - val_accuracy: 0.0984\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1009.1812 - accuracy: 0.1018 - val_loss: 590862016.0000 - val_accuracy: 0.1008\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 1367.6714 - accuracy: 0.0993 - val_loss: 854473344.0000 - val_accuracy: 0.1010\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 636.2495 - accuracy: 0.1029 - val_loss: 996233408.0000 - val_accuracy: 0.0981\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 533.8175 - accuracy: 0.1024 - val_loss: 997641216.0000 - val_accuracy: 0.0947\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 586.3162 - accuracy: 0.0997 - val_loss: 982273728.0000 - val_accuracy: 0.0956\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 697.3795 - accuracy: 0.0981 - val_loss: 979281536.0000 - val_accuracy: 0.1006\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 612.0562 - accuracy: 0.1002 - val_loss: 979232128.0000 - val_accuracy: 0.1008\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2561.5701 - accuracy: 0.1005 - val_loss: 931566976.0000 - val_accuracy: 0.0948\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 4114.1899 - accuracy: 0.1021 - val_loss: 1471192064.0000 - val_accuracy: 0.1006\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 6949.0186 - accuracy: 0.0979 - val_loss: 862473664.0000 - val_accuracy: 0.0971\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1862.4926 - accuracy: 0.1024 - val_loss: 640908224.0000 - val_accuracy: 0.1058\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1025.4817 - accuracy: 0.0985 - val_loss: 526677952.0000 - val_accuracy: 0.0961\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 794.4190 - accuracy: 0.1005 - val_loss: 402712384.0000 - val_accuracy: 0.1009\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1024.9564 - accuracy: 0.1005 - val_loss: 434922656.0000 - val_accuracy: 0.1020\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 650.0650 - accuracy: 0.1026 - val_loss: 377967040.0000 - val_accuracy: 0.0989\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1107.3770 - accuracy: 0.0986 - val_loss: 395750080.0000 - val_accuracy: 0.1033\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 694.4973 - accuracy: 0.1029 - val_loss: 360807392.0000 - val_accuracy: 0.1002\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 922.4834 - accuracy: 0.1010 - val_loss: 261808864.0000 - val_accuracy: 0.1016\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 698.9438 - accuracy: 0.0961 - val_loss: 279743904.0000 - val_accuracy: 0.1008\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 649.3064 - accuracy: 0.0995 - val_loss: 261323920.0000 - val_accuracy: 0.1014\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 495.1156 - accuracy: 0.1007 - val_loss: 253607712.0000 - val_accuracy: 0.0955\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 760.7365 - accuracy: 0.0974 - val_loss: 220104048.0000 - val_accuracy: 0.0994\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 850.4587 - accuracy: 0.1015 - val_loss: 189738768.0000 - val_accuracy: 0.1002\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 694.3392 - accuracy: 0.1018 - val_loss: 181074192.0000 - val_accuracy: 0.1022\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 636.2824 - accuracy: 0.1006 - val_loss: 203117952.0000 - val_accuracy: 0.0967\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 994.3999 - accuracy: 0.0995 - val_loss: 351629728.0000 - val_accuracy: 0.1022\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 13013.1230 - accuracy: 0.1013 - val_loss: 311101440.0000 - val_accuracy: 0.1004\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 3045.4683 - accuracy: 0.0996 - val_loss: 228619536.0000 - val_accuracy: 0.0961\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 477.3477 - accuracy: 0.1004 - val_loss: 176907024.0000 - val_accuracy: 0.0986\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 484.7115 - accuracy: 0.0994 - val_loss: 163141056.0000 - val_accuracy: 0.1001\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 953.1890 - accuracy: 0.1011 - val_loss: 182182896.0000 - val_accuracy: 0.1013\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1258.3701 - accuracy: 0.1012 - val_loss: 153619744.0000 - val_accuracy: 0.1014\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1130.0288 - accuracy: 0.1003 - val_loss: 236898432.0000 - val_accuracy: 0.0971\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 4050.4207 - accuracy: 0.1004 - val_loss: 306241184.0000 - val_accuracy: 0.0959\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 19165.0977 - accuracy: 0.1020 - val_loss: 138671376.0000 - val_accuracy: 0.1033\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 5223.9258 - accuracy: 0.0974 - val_loss: 230228768.0000 - val_accuracy: 0.1052\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 601.8851 - accuracy: 0.1002 - val_loss: 193128896.0000 - val_accuracy: 0.1006\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 1130.4894 - accuracy: 0.0998 - val_loss: 181648096.0000 - val_accuracy: 0.1002\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 582.8974 - accuracy: 0.0978 - val_loss: 148711840.0000 - val_accuracy: 0.1002\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 720.2215 - accuracy: 0.1001 - val_loss: 184604640.0000 - val_accuracy: 0.1008\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 861.8176 - accuracy: 0.1017 - val_loss: 118882576.0000 - val_accuracy: 0.1017\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 584.0073 - accuracy: 0.1002 - val_loss: 110943752.0000 - val_accuracy: 0.1017\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 577.6824 - accuracy: 0.0999 - val_loss: 107651576.0000 - val_accuracy: 0.0982\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 586.8148 - accuracy: 0.1014 - val_loss: 105775608.0000 - val_accuracy: 0.1007\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 476.2455 - accuracy: 0.1018 - val_loss: 104670136.0000 - val_accuracy: 0.1016\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 496.8627 - accuracy: 0.0999 - val_loss: 104109568.0000 - val_accuracy: 0.1016\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 646.7184 - accuracy: 0.1013 - val_loss: 103902152.0000 - val_accuracy: 0.0955\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 694.1942 - accuracy: 0.1009 - val_loss: 103879120.0000 - val_accuracy: 0.0982\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 606.0596 - accuracy: 0.0998 - val_loss: 104017272.0000 - val_accuracy: 0.1002\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 720.6567 - accuracy: 0.1008 - val_loss: 104340856.0000 - val_accuracy: 0.0955\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 689.1813 - accuracy: 0.0985 - val_loss: 145399888.0000 - val_accuracy: 0.0989\n",
      "Epoch 67/100\n",
      "11/42 [======>.......................] - ETA: 1s - loss: 857.0512 - accuracy: 0.0948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5a7f53664aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-03cb08865d7c>\u001b[0m in \u001b[0;36mmodel_7\u001b[0;34m(beta1, beta2, lr)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mada_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m#callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary of model\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mRES_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_7(0.9, 0.99, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The loss is too high lets increase learning rate to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "t1_hqIF4ED-j",
    "outputId": "c7b26c7f-6cb2-493f-dde6-d3dcb643776e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 6.2197 - accuracy: 0.1011 - val_loss: 2118.6411 - val_accuracy: 0.0971\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3118 - accuracy: 0.1002 - val_loss: 63.2904 - val_accuracy: 0.0981\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3119 - accuracy: 0.0994 - val_loss: 3.8652 - val_accuracy: 0.1017\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3096 - accuracy: 0.1010 - val_loss: 2.4654 - val_accuracy: 0.1017\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3126 - accuracy: 0.1018 - val_loss: 2.4482 - val_accuracy: 0.1002\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3167 - accuracy: 0.1002 - val_loss: 2.4692 - val_accuracy: 0.1002\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3119 - accuracy: 0.0981 - val_loss: 2.5161 - val_accuracy: 0.1007\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3173 - accuracy: 0.0984 - val_loss: 2.5772 - val_accuracy: 0.0955\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3151 - accuracy: 0.0985 - val_loss: 2.6538 - val_accuracy: 0.0982\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3206 - accuracy: 0.1007 - val_loss: 2.7137 - val_accuracy: 0.1018\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3163 - accuracy: 0.0976 - val_loss: 2.7850 - val_accuracy: 0.1004\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3215 - accuracy: 0.1007 - val_loss: 2.8786 - val_accuracy: 0.1006\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3191 - accuracy: 0.0985 - val_loss: 2.9790 - val_accuracy: 0.1007\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3162 - accuracy: 0.0989 - val_loss: 3.0920 - val_accuracy: 0.1015\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3181 - accuracy: 0.1004 - val_loss: 3.2428 - val_accuracy: 0.1002\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3240 - accuracy: 0.1025 - val_loss: 22.4419 - val_accuracy: 0.0954\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3385 - accuracy: 0.0985 - val_loss: 46.6423 - val_accuracy: 0.0985\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3176 - accuracy: 0.1016 - val_loss: 5.3247 - val_accuracy: 0.0997\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3218 - accuracy: 0.0999 - val_loss: 2.9066 - val_accuracy: 0.1011\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3181 - accuracy: 0.1004 - val_loss: 2.7153 - val_accuracy: 0.1000\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3166 - accuracy: 0.0978 - val_loss: 2.6814 - val_accuracy: 0.1002\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3128 - accuracy: 0.0982 - val_loss: 2.6536 - val_accuracy: 0.1002\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3202 - accuracy: 0.0989 - val_loss: 2.6536 - val_accuracy: 0.0955\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3187 - accuracy: 0.1002 - val_loss: 2.6589 - val_accuracy: 0.0982\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3197 - accuracy: 0.0969 - val_loss: 13.6673 - val_accuracy: 0.1018\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3115 - accuracy: 0.0980 - val_loss: 2.6426 - val_accuracy: 0.1002\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3161 - accuracy: 0.1014 - val_loss: 2.6463 - val_accuracy: 0.1004\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3181 - accuracy: 0.0985 - val_loss: 2.6309 - val_accuracy: 0.1006\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3158 - accuracy: 0.0974 - val_loss: 2.6374 - val_accuracy: 0.1002\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3161 - accuracy: 0.0989 - val_loss: 2.6406 - val_accuracy: 0.0955\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3189 - accuracy: 0.0996 - val_loss: 2.6366 - val_accuracy: 0.1015\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3194 - accuracy: 0.0986 - val_loss: 2.6347 - val_accuracy: 0.1015\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3200 - accuracy: 0.0987 - val_loss: 2.6506 - val_accuracy: 0.1002\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3163 - accuracy: 0.0993 - val_loss: 2.6341 - val_accuracy: 0.1002\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3128 - accuracy: 0.1018 - val_loss: 2.6257 - val_accuracy: 0.1002\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.3136 - accuracy: 0.0980 - val_loss: 2.6354 - val_accuracy: 0.0982\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3150 - accuracy: 0.1005 - val_loss: 2.6272 - val_accuracy: 0.1006\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.3134 - accuracy: 0.0991 - val_loss: 2.6395 - val_accuracy: 0.1007\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3165 - accuracy: 0.0982 - val_loss: 2.6387 - val_accuracy: 0.1004\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3196 - accuracy: 0.1012 - val_loss: 2.6282 - val_accuracy: 0.1015\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3130 - accuracy: 0.1014 - val_loss: 2.6353 - val_accuracy: 0.1007\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3175 - accuracy: 0.0995 - val_loss: 2.6260 - val_accuracy: 0.1006\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3133 - accuracy: 0.1001 - val_loss: 2.6306 - val_accuracy: 0.1004\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3187 - accuracy: 0.1013 - val_loss: 2.6507 - val_accuracy: 0.0955\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3210 - accuracy: 0.1009 - val_loss: 2.6359 - val_accuracy: 0.1006\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3170 - accuracy: 0.0991 - val_loss: 2.6282 - val_accuracy: 0.1004\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3139 - accuracy: 0.0991 - val_loss: 2.6320 - val_accuracy: 0.1002\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3143 - accuracy: 0.0997 - val_loss: 2.6259 - val_accuracy: 0.1007\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3137 - accuracy: 0.1010 - val_loss: 2.6553 - val_accuracy: 0.1004\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3360 - accuracy: 0.1016 - val_loss: 107.3845 - val_accuracy: 0.1029\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3119 - accuracy: 0.0999 - val_loss: 24.9064 - val_accuracy: 0.0959\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3142 - accuracy: 0.0992 - val_loss: 3.9247 - val_accuracy: 0.0954\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3140 - accuracy: 0.0989 - val_loss: 2.5941 - val_accuracy: 0.1015\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3157 - accuracy: 0.0993 - val_loss: 2.5472 - val_accuracy: 0.1006\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3149 - accuracy: 0.1021 - val_loss: 2.5039 - val_accuracy: 0.1007\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3162 - accuracy: 0.0991 - val_loss: 2.5126 - val_accuracy: 0.1002\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3176 - accuracy: 0.0985 - val_loss: 2.4827 - val_accuracy: 0.1007\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3145 - accuracy: 0.1008 - val_loss: 2.4888 - val_accuracy: 0.1002\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3187 - accuracy: 0.1014 - val_loss: 2.4875 - val_accuracy: 0.0982\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3180 - accuracy: 0.0996 - val_loss: 2.4742 - val_accuracy: 0.1002\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3177 - accuracy: 0.0994 - val_loss: 2.5025 - val_accuracy: 0.1015\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3206 - accuracy: 0.0995 - val_loss: 3.2814 - val_accuracy: 0.0954\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3190 - accuracy: 0.1002 - val_loss: 2.4377 - val_accuracy: 0.1004\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3186 - accuracy: 0.0989 - val_loss: 2.3543 - val_accuracy: 0.1007\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3190 - accuracy: 0.0995 - val_loss: 2.3493 - val_accuracy: 0.1015\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3166 - accuracy: 0.1024 - val_loss: 2.3408 - val_accuracy: 0.1004\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3159 - accuracy: 0.1010 - val_loss: 2.3486 - val_accuracy: 0.1006\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3194 - accuracy: 0.0997 - val_loss: 2.3420 - val_accuracy: 0.1015\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3179 - accuracy: 0.0998 - val_loss: 2.3391 - val_accuracy: 0.0982\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3158 - accuracy: 0.1004 - val_loss: 2.3354 - val_accuracy: 0.1007\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 2.3166 - accuracy: 0.1001 - val_loss: 2.3502 - val_accuracy: 0.1006\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3155 - accuracy: 0.0993 - val_loss: 2.3396 - val_accuracy: 0.1006\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3154 - accuracy: 0.1002 - val_loss: 2.3476 - val_accuracy: 0.1015\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 2.3160 - accuracy: 0.0969 - val_loss: 2.3429 - val_accuracy: 0.0982\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3151 - accuracy: 0.1005 - val_loss: 2.3419 - val_accuracy: 0.0955\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3144 - accuracy: 0.1014 - val_loss: 2.3460 - val_accuracy: 0.0982\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3186 - accuracy: 0.0999 - val_loss: 2.3398 - val_accuracy: 0.1004\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3141 - accuracy: 0.1000 - val_loss: 2.3513 - val_accuracy: 0.0955\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3191 - accuracy: 0.0975 - val_loss: 2.3585 - val_accuracy: 0.0955\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3164 - accuracy: 0.1033 - val_loss: 2.3334 - val_accuracy: 0.1015\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3165 - accuracy: 0.1001 - val_loss: 2.3378 - val_accuracy: 0.1004\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3200 - accuracy: 0.0998 - val_loss: 2.3446 - val_accuracy: 0.1004\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3193 - accuracy: 0.0974 - val_loss: 2.3427 - val_accuracy: 0.1002\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3116 - accuracy: 0.1006 - val_loss: 2.3362 - val_accuracy: 0.1007\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3125 - accuracy: 0.1011 - val_loss: 2.3513 - val_accuracy: 0.1007\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3183 - accuracy: 0.1005 - val_loss: 2.3361 - val_accuracy: 0.1018\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3184 - accuracy: 0.1006 - val_loss: 2.3474 - val_accuracy: 0.0982\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3185 - accuracy: 0.1000 - val_loss: 2.3380 - val_accuracy: 0.1018\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3217 - accuracy: 0.1023 - val_loss: 2.3414 - val_accuracy: 0.1007\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3168 - accuracy: 0.1023 - val_loss: 2.3428 - val_accuracy: 0.1007\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3192 - accuracy: 0.1011 - val_loss: 2.3419 - val_accuracy: 0.1015\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3122 - accuracy: 0.1024 - val_loss: 2.3320 - val_accuracy: 0.1002\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3149 - accuracy: 0.0981 - val_loss: 2.3422 - val_accuracy: 0.1007\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3133 - accuracy: 0.0990 - val_loss: 2.3402 - val_accuracy: 0.1002\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3184 - accuracy: 0.1003 - val_loss: 2.3424 - val_accuracy: 0.1002\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3186 - accuracy: 0.0987 - val_loss: 2.3477 - val_accuracy: 0.1007\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3171 - accuracy: 0.0985 - val_loss: 2.3414 - val_accuracy: 0.1007\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3155 - accuracy: 0.0994 - val_loss: 2.3399 - val_accuracy: 0.1007\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3172 - accuracy: 0.1001 - val_loss: 2.3484 - val_accuracy: 0.1006\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3181 - accuracy: 0.1006 - val_loss: 2.3440 - val_accuracy: 0.1018\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 2.3440 - accuracy: 0.1018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f51bdc196d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7(0.9, 0.99, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The test accuracy is not good . Lets reduce the learning rate to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TJaqlwZtaeJu",
    "outputId": "e79d586e-6da7-406f-e015-b183c05e1b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 2.3988 - accuracy: 0.1588 - val_loss: 2.2835 - val_accuracy: 0.1361\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.0265 - accuracy: 0.2965 - val_loss: 2.2152 - val_accuracy: 0.2660\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1.7337 - accuracy: 0.4247 - val_loss: 2.1120 - val_accuracy: 0.3476\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 1.5156 - accuracy: 0.5132 - val_loss: 1.9162 - val_accuracy: 0.5012\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1.3490 - accuracy: 0.5752 - val_loss: 1.7277 - val_accuracy: 0.5732\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1.2334 - accuracy: 0.6171 - val_loss: 1.5343 - val_accuracy: 0.6282\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1.1435 - accuracy: 0.6486 - val_loss: 1.3377 - val_accuracy: 0.6672\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 1.0759 - accuracy: 0.6709 - val_loss: 1.2017 - val_accuracy: 0.6822\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 1.0236 - accuracy: 0.6870 - val_loss: 1.0714 - val_accuracy: 0.7145\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.9725 - accuracy: 0.7010 - val_loss: 1.0229 - val_accuracy: 0.7130\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 61ms/step - loss: 0.9362 - accuracy: 0.7134 - val_loss: 0.9536 - val_accuracy: 0.7246\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.8986 - accuracy: 0.7249 - val_loss: 0.8777 - val_accuracy: 0.7456\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.8690 - accuracy: 0.7329 - val_loss: 0.8368 - val_accuracy: 0.7531\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.8413 - accuracy: 0.7442 - val_loss: 0.8250 - val_accuracy: 0.7507\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.8132 - accuracy: 0.7515 - val_loss: 0.8106 - val_accuracy: 0.7561\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.7919 - accuracy: 0.7575 - val_loss: 0.7738 - val_accuracy: 0.7666\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.7724 - accuracy: 0.7623 - val_loss: 0.7592 - val_accuracy: 0.7748\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.7547 - accuracy: 0.7694 - val_loss: 0.7406 - val_accuracy: 0.7797\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.7383 - accuracy: 0.7747 - val_loss: 0.7162 - val_accuracy: 0.7866\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.7211 - accuracy: 0.7780 - val_loss: 0.7021 - val_accuracy: 0.7891\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.7080 - accuracy: 0.7822 - val_loss: 0.7061 - val_accuracy: 0.7877\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6916 - accuracy: 0.7880 - val_loss: 0.6934 - val_accuracy: 0.7885\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6817 - accuracy: 0.7928 - val_loss: 0.6885 - val_accuracy: 0.7936\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.6669 - accuracy: 0.7950 - val_loss: 0.6797 - val_accuracy: 0.7948\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6562 - accuracy: 0.7985 - val_loss: 0.6551 - val_accuracy: 0.8041\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.6457 - accuracy: 0.8010 - val_loss: 0.6521 - val_accuracy: 0.8049\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6379 - accuracy: 0.8028 - val_loss: 0.6620 - val_accuracy: 0.7999\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.6257 - accuracy: 0.8059 - val_loss: 0.6390 - val_accuracy: 0.8073\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6155 - accuracy: 0.8102 - val_loss: 0.6613 - val_accuracy: 0.8028\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6096 - accuracy: 0.8126 - val_loss: 0.6284 - val_accuracy: 0.8121\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.6012 - accuracy: 0.8154 - val_loss: 0.6342 - val_accuracy: 0.8099\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5918 - accuracy: 0.8172 - val_loss: 0.6193 - val_accuracy: 0.8167\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5847 - accuracy: 0.8198 - val_loss: 0.5995 - val_accuracy: 0.8217\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5779 - accuracy: 0.8212 - val_loss: 0.5940 - val_accuracy: 0.8232\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5726 - accuracy: 0.8219 - val_loss: 0.5910 - val_accuracy: 0.8233\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5622 - accuracy: 0.8254 - val_loss: 0.6069 - val_accuracy: 0.8150\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5545 - accuracy: 0.8289 - val_loss: 0.6201 - val_accuracy: 0.8114\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5507 - accuracy: 0.8315 - val_loss: 0.5918 - val_accuracy: 0.8239\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5469 - accuracy: 0.8298 - val_loss: 0.5928 - val_accuracy: 0.8228\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5380 - accuracy: 0.8326 - val_loss: 0.5956 - val_accuracy: 0.8199\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5340 - accuracy: 0.8346 - val_loss: 0.6195 - val_accuracy: 0.8107\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5329 - accuracy: 0.8357 - val_loss: 0.5759 - val_accuracy: 0.8278\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5214 - accuracy: 0.8408 - val_loss: 0.5711 - val_accuracy: 0.8271\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.5163 - accuracy: 0.8399 - val_loss: 0.5660 - val_accuracy: 0.8287\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5083 - accuracy: 0.8416 - val_loss: 0.5726 - val_accuracy: 0.8292\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5079 - accuracy: 0.8444 - val_loss: 0.5663 - val_accuracy: 0.8301\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.5038 - accuracy: 0.8443 - val_loss: 0.5652 - val_accuracy: 0.8299\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4976 - accuracy: 0.8464 - val_loss: 0.5670 - val_accuracy: 0.8297\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4954 - accuracy: 0.8475 - val_loss: 0.5876 - val_accuracy: 0.8204\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4875 - accuracy: 0.8488 - val_loss: 0.5763 - val_accuracy: 0.8278\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4869 - accuracy: 0.8478 - val_loss: 0.5751 - val_accuracy: 0.8249\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4818 - accuracy: 0.8518 - val_loss: 0.5519 - val_accuracy: 0.8349\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4736 - accuracy: 0.8518 - val_loss: 0.5501 - val_accuracy: 0.8359\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4728 - accuracy: 0.8528 - val_loss: 0.5676 - val_accuracy: 0.8271\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4662 - accuracy: 0.8560 - val_loss: 0.5437 - val_accuracy: 0.8389\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4661 - accuracy: 0.8548 - val_loss: 0.5490 - val_accuracy: 0.8364\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4600 - accuracy: 0.8570 - val_loss: 0.5543 - val_accuracy: 0.8338\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4629 - accuracy: 0.8562 - val_loss: 0.5650 - val_accuracy: 0.8316\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4582 - accuracy: 0.8568 - val_loss: 0.5618 - val_accuracy: 0.8327\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4497 - accuracy: 0.8608 - val_loss: 0.5510 - val_accuracy: 0.8343\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4514 - accuracy: 0.8595 - val_loss: 0.5350 - val_accuracy: 0.8409\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4438 - accuracy: 0.8619 - val_loss: 0.5390 - val_accuracy: 0.8387\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4435 - accuracy: 0.8623 - val_loss: 0.5510 - val_accuracy: 0.8344\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4373 - accuracy: 0.8617 - val_loss: 0.5587 - val_accuracy: 0.8307\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4318 - accuracy: 0.8653 - val_loss: 0.5570 - val_accuracy: 0.8331\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4351 - accuracy: 0.8635 - val_loss: 0.5314 - val_accuracy: 0.8416\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4274 - accuracy: 0.8652 - val_loss: 0.5405 - val_accuracy: 0.8381\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4272 - accuracy: 0.8670 - val_loss: 0.5370 - val_accuracy: 0.8378\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.4219 - accuracy: 0.8674 - val_loss: 0.5532 - val_accuracy: 0.8334\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4216 - accuracy: 0.8680 - val_loss: 0.5383 - val_accuracy: 0.8386\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4194 - accuracy: 0.8691 - val_loss: 0.5401 - val_accuracy: 0.8383\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4160 - accuracy: 0.8699 - val_loss: 0.5302 - val_accuracy: 0.8410\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4161 - accuracy: 0.8680 - val_loss: 0.5261 - val_accuracy: 0.8438\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4112 - accuracy: 0.8696 - val_loss: 0.5461 - val_accuracy: 0.8361\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4118 - accuracy: 0.8731 - val_loss: 0.5293 - val_accuracy: 0.8431\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4083 - accuracy: 0.8728 - val_loss: 0.5542 - val_accuracy: 0.8319\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4050 - accuracy: 0.8717 - val_loss: 0.5400 - val_accuracy: 0.8411\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.4008 - accuracy: 0.8738 - val_loss: 0.5224 - val_accuracy: 0.8449\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3987 - accuracy: 0.8735 - val_loss: 0.5209 - val_accuracy: 0.8436\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3924 - accuracy: 0.8764 - val_loss: 0.5303 - val_accuracy: 0.8432\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3952 - accuracy: 0.8775 - val_loss: 0.5075 - val_accuracy: 0.8503\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3887 - accuracy: 0.8775 - val_loss: 0.5362 - val_accuracy: 0.8396\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3910 - accuracy: 0.8773 - val_loss: 0.5376 - val_accuracy: 0.8403\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3910 - accuracy: 0.8773 - val_loss: 0.5490 - val_accuracy: 0.8361\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3844 - accuracy: 0.8775 - val_loss: 0.5381 - val_accuracy: 0.8406\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 68ms/step - loss: 0.3812 - accuracy: 0.8796 - val_loss: 0.5137 - val_accuracy: 0.8481\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3806 - accuracy: 0.8799 - val_loss: 0.5113 - val_accuracy: 0.8468\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3792 - accuracy: 0.8814 - val_loss: 0.5367 - val_accuracy: 0.8434\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3739 - accuracy: 0.8829 - val_loss: 0.5391 - val_accuracy: 0.8408\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3763 - accuracy: 0.8801 - val_loss: 0.5094 - val_accuracy: 0.8494\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3696 - accuracy: 0.8830 - val_loss: 0.5234 - val_accuracy: 0.8442\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3708 - accuracy: 0.8816 - val_loss: 0.5567 - val_accuracy: 0.8349\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3700 - accuracy: 0.8830 - val_loss: 0.5152 - val_accuracy: 0.8449\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3635 - accuracy: 0.8858 - val_loss: 0.5370 - val_accuracy: 0.8401\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3646 - accuracy: 0.8844 - val_loss: 0.5226 - val_accuracy: 0.8451\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3646 - accuracy: 0.8832 - val_loss: 0.5160 - val_accuracy: 0.8476\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3593 - accuracy: 0.8862 - val_loss: 0.5249 - val_accuracy: 0.8450\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3575 - accuracy: 0.8866 - val_loss: 0.5052 - val_accuracy: 0.8530\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3565 - accuracy: 0.8869 - val_loss: 0.5224 - val_accuracy: 0.8448\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 0.3581 - accuracy: 0.8869 - val_loss: 0.5490 - val_accuracy: 0.8409\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f51b9fd3fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7(0.9, 0.99, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IfPvWAMaAMYA",
    "outputId": "83c843de-4412-4bc0-b68c-a66d5003dec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 2.7498 - accuracy: 0.1025 - val_loss: 2.3187 - val_accuracy: 0.1020\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.7326 - accuracy: 0.1043 - val_loss: 2.3193 - val_accuracy: 0.1013\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.7244 - accuracy: 0.1034 - val_loss: 2.3256 - val_accuracy: 0.1018\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.7119 - accuracy: 0.1063 - val_loss: 2.3341 - val_accuracy: 0.1016\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.7052 - accuracy: 0.1061 - val_loss: 2.3442 - val_accuracy: 0.1016\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6883 - accuracy: 0.1104 - val_loss: 2.3558 - val_accuracy: 0.1051\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6803 - accuracy: 0.1126 - val_loss: 2.3682 - val_accuracy: 0.1070\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6700 - accuracy: 0.1122 - val_loss: 2.3811 - val_accuracy: 0.1102\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.6595 - accuracy: 0.1143 - val_loss: 2.3941 - val_accuracy: 0.1130\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.6490 - accuracy: 0.1163 - val_loss: 2.4063 - val_accuracy: 0.1164\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.6379 - accuracy: 0.1165 - val_loss: 2.4165 - val_accuracy: 0.1202\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6330 - accuracy: 0.1179 - val_loss: 2.4250 - val_accuracy: 0.1230\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6235 - accuracy: 0.1200 - val_loss: 2.4315 - val_accuracy: 0.1264\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6162 - accuracy: 0.1214 - val_loss: 2.4363 - val_accuracy: 0.1271\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.6082 - accuracy: 0.1223 - val_loss: 2.4393 - val_accuracy: 0.1284\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.6004 - accuracy: 0.1234 - val_loss: 2.4405 - val_accuracy: 0.1307\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.5892 - accuracy: 0.1229 - val_loss: 2.4399 - val_accuracy: 0.1325\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5829 - accuracy: 0.1254 - val_loss: 2.4381 - val_accuracy: 0.1349\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5750 - accuracy: 0.1250 - val_loss: 2.4343 - val_accuracy: 0.1376\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5680 - accuracy: 0.1281 - val_loss: 2.4294 - val_accuracy: 0.1392\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.5624 - accuracy: 0.1303 - val_loss: 2.4239 - val_accuracy: 0.1413\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5523 - accuracy: 0.1302 - val_loss: 2.4179 - val_accuracy: 0.1434\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.5470 - accuracy: 0.1320 - val_loss: 2.4132 - val_accuracy: 0.1454\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.5387 - accuracy: 0.1343 - val_loss: 2.4078 - val_accuracy: 0.1478\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.5306 - accuracy: 0.1351 - val_loss: 2.4025 - val_accuracy: 0.1491\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5169 - accuracy: 0.1382 - val_loss: 2.3969 - val_accuracy: 0.1509\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.5154 - accuracy: 0.1374 - val_loss: 2.3905 - val_accuracy: 0.1523\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.5166 - accuracy: 0.1357 - val_loss: 2.3849 - val_accuracy: 0.1546\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.5086 - accuracy: 0.1371 - val_loss: 2.3795 - val_accuracy: 0.1557\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.5009 - accuracy: 0.1414 - val_loss: 2.3733 - val_accuracy: 0.1578\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4899 - accuracy: 0.1442 - val_loss: 2.3675 - val_accuracy: 0.1584\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.4852 - accuracy: 0.1440 - val_loss: 2.3615 - val_accuracy: 0.1600\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.4845 - accuracy: 0.1454 - val_loss: 2.3561 - val_accuracy: 0.1617\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4730 - accuracy: 0.1486 - val_loss: 2.3507 - val_accuracy: 0.1644\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4698 - accuracy: 0.1501 - val_loss: 2.3456 - val_accuracy: 0.1664\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.4668 - accuracy: 0.1500 - val_loss: 2.3400 - val_accuracy: 0.1688\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4582 - accuracy: 0.1515 - val_loss: 2.3345 - val_accuracy: 0.1707\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4515 - accuracy: 0.1532 - val_loss: 2.3292 - val_accuracy: 0.1718\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4440 - accuracy: 0.1537 - val_loss: 2.3246 - val_accuracy: 0.1736\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4443 - accuracy: 0.1531 - val_loss: 2.3189 - val_accuracy: 0.1772\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4399 - accuracy: 0.1562 - val_loss: 2.3139 - val_accuracy: 0.1792\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4351 - accuracy: 0.1570 - val_loss: 2.3091 - val_accuracy: 0.1811\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4227 - accuracy: 0.1589 - val_loss: 2.3041 - val_accuracy: 0.1822\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4187 - accuracy: 0.1590 - val_loss: 2.2991 - val_accuracy: 0.1844\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.4141 - accuracy: 0.1649 - val_loss: 2.2942 - val_accuracy: 0.1869\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.4111 - accuracy: 0.1630 - val_loss: 2.2889 - val_accuracy: 0.1883\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3991 - accuracy: 0.1651 - val_loss: 2.2841 - val_accuracy: 0.1906\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3975 - accuracy: 0.1643 - val_loss: 2.2788 - val_accuracy: 0.1928\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3951 - accuracy: 0.1664 - val_loss: 2.2744 - val_accuracy: 0.1946\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3920 - accuracy: 0.1657 - val_loss: 2.2696 - val_accuracy: 0.1975\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3870 - accuracy: 0.1695 - val_loss: 2.2645 - val_accuracy: 0.1991\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3785 - accuracy: 0.1721 - val_loss: 2.2601 - val_accuracy: 0.2002\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3730 - accuracy: 0.1729 - val_loss: 2.2555 - val_accuracy: 0.2036\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.3694 - accuracy: 0.1765 - val_loss: 2.2506 - val_accuracy: 0.2055\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3683 - accuracy: 0.1752 - val_loss: 2.2456 - val_accuracy: 0.2071\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3587 - accuracy: 0.1767 - val_loss: 2.2412 - val_accuracy: 0.2095\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3540 - accuracy: 0.1800 - val_loss: 2.2367 - val_accuracy: 0.2113\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3476 - accuracy: 0.1801 - val_loss: 2.2323 - val_accuracy: 0.2128\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3452 - accuracy: 0.1802 - val_loss: 2.2278 - val_accuracy: 0.2149\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3430 - accuracy: 0.1815 - val_loss: 2.2233 - val_accuracy: 0.2173\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3388 - accuracy: 0.1818 - val_loss: 2.2186 - val_accuracy: 0.2198\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3327 - accuracy: 0.1862 - val_loss: 2.2139 - val_accuracy: 0.2219\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3260 - accuracy: 0.1898 - val_loss: 2.2097 - val_accuracy: 0.2244\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3241 - accuracy: 0.1917 - val_loss: 2.2053 - val_accuracy: 0.2264\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.3196 - accuracy: 0.1901 - val_loss: 2.2010 - val_accuracy: 0.2287\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3121 - accuracy: 0.1925 - val_loss: 2.1971 - val_accuracy: 0.2293\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3113 - accuracy: 0.1933 - val_loss: 2.1923 - val_accuracy: 0.2318\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.3033 - accuracy: 0.1946 - val_loss: 2.1883 - val_accuracy: 0.2335\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2982 - accuracy: 0.1960 - val_loss: 2.1840 - val_accuracy: 0.2359\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2969 - accuracy: 0.1966 - val_loss: 2.1797 - val_accuracy: 0.2382\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2900 - accuracy: 0.2023 - val_loss: 2.1754 - val_accuracy: 0.2404\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2857 - accuracy: 0.2019 - val_loss: 2.1709 - val_accuracy: 0.2426\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2819 - accuracy: 0.2018 - val_loss: 2.1662 - val_accuracy: 0.2466\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.2755 - accuracy: 0.2050 - val_loss: 2.1622 - val_accuracy: 0.2492\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2690 - accuracy: 0.2066 - val_loss: 2.1587 - val_accuracy: 0.2504\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2719 - accuracy: 0.2064 - val_loss: 2.1542 - val_accuracy: 0.2516\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.2671 - accuracy: 0.2076 - val_loss: 2.1500 - val_accuracy: 0.2541\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2557 - accuracy: 0.2121 - val_loss: 2.1461 - val_accuracy: 0.2550\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2535 - accuracy: 0.2147 - val_loss: 2.1423 - val_accuracy: 0.2564\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 2.2455 - accuracy: 0.2164 - val_loss: 2.1377 - val_accuracy: 0.2587\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.2523 - accuracy: 0.2125 - val_loss: 2.1330 - val_accuracy: 0.2612\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 2.2436 - accuracy: 0.2155 - val_loss: 2.1292 - val_accuracy: 0.2628\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2355 - accuracy: 0.2188 - val_loss: 2.1255 - val_accuracy: 0.2658\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2366 - accuracy: 0.2212 - val_loss: 2.1211 - val_accuracy: 0.2674\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2334 - accuracy: 0.2183 - val_loss: 2.1171 - val_accuracy: 0.2689\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2249 - accuracy: 0.2239 - val_loss: 2.1133 - val_accuracy: 0.2706\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.2195 - accuracy: 0.2276 - val_loss: 2.1093 - val_accuracy: 0.2729\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.2179 - accuracy: 0.2257 - val_loss: 2.1050 - val_accuracy: 0.2763\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2142 - accuracy: 0.2270 - val_loss: 2.1004 - val_accuracy: 0.2789\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2130 - accuracy: 0.2280 - val_loss: 2.0966 - val_accuracy: 0.2803\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2072 - accuracy: 0.2292 - val_loss: 2.0928 - val_accuracy: 0.2826\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.2050 - accuracy: 0.2324 - val_loss: 2.0888 - val_accuracy: 0.2841\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.1945 - accuracy: 0.2347 - val_loss: 2.0850 - val_accuracy: 0.2862\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.1906 - accuracy: 0.2358 - val_loss: 2.0812 - val_accuracy: 0.2882\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.1925 - accuracy: 0.2357 - val_loss: 2.0767 - val_accuracy: 0.2906\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.1867 - accuracy: 0.2390 - val_loss: 2.0726 - val_accuracy: 0.2919\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.1775 - accuracy: 0.2396 - val_loss: 2.0685 - val_accuracy: 0.2938\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.1735 - accuracy: 0.2455 - val_loss: 2.0643 - val_accuracy: 0.2958\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 2.1746 - accuracy: 0.2423 - val_loss: 2.0602 - val_accuracy: 0.2978\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 62ms/step - loss: 2.1701 - accuracy: 0.2431 - val_loss: 2.0564 - val_accuracy: 0.3004\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 2.0564 - accuracy: 0.3004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f51b8462588>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7(0.9, 0.99, 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmCAoGu8Fm6I"
   },
   "source": [
    "So we see that when the learning rate is between 0.1 to 0.0001 the test accuracy is coming above 79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cS2cbdUGFHO1"
   },
   "source": [
    "So lets  coarse search for 5 times with learning rate between 0.10 and 0.0001 for 100 epochs.We will randomly pick four values between 0.1 and 0.0001 and then calculate accuracy for each lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-G2HtVhgIAxP"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wGe92JZ1C7nl",
    "outputId": "f75d3c99-e324-42e9-c872-eb44e05e96e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 70ms/step - loss: 2.4250 - accuracy: 0.1582 - val_loss: 5.2255 - val_accuracy: 0.1928\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 1.6515 - accuracy: 0.4000 - val_loss: 2.5735 - val_accuracy: 0.3752\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 1.2446 - accuracy: 0.5878 - val_loss: 1.4348 - val_accuracy: 0.5491\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 1.0560 - accuracy: 0.6594 - val_loss: 1.7865 - val_accuracy: 0.4658\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.9438 - accuracy: 0.6976 - val_loss: 1.1101 - val_accuracy: 0.6373\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.8611 - accuracy: 0.7238 - val_loss: 1.3646 - val_accuracy: 0.5554\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.8011 - accuracy: 0.7439 - val_loss: 1.2014 - val_accuracy: 0.6151\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.7690 - accuracy: 0.7561 - val_loss: 1.1097 - val_accuracy: 0.6372\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.7201 - accuracy: 0.7708 - val_loss: 0.9066 - val_accuracy: 0.7184\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6905 - accuracy: 0.7803 - val_loss: 1.0202 - val_accuracy: 0.6763\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6568 - accuracy: 0.7910 - val_loss: 1.0698 - val_accuracy: 0.6811\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.6373 - accuracy: 0.7969 - val_loss: 0.8604 - val_accuracy: 0.7279\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6170 - accuracy: 0.8039 - val_loss: 0.9503 - val_accuracy: 0.7082\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5889 - accuracy: 0.8114 - val_loss: 0.7516 - val_accuracy: 0.7649\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5873 - accuracy: 0.8127 - val_loss: 1.1241 - val_accuracy: 0.6676\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5707 - accuracy: 0.8176 - val_loss: 0.7753 - val_accuracy: 0.7573\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5520 - accuracy: 0.8230 - val_loss: 0.8499 - val_accuracy: 0.7484\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5448 - accuracy: 0.8249 - val_loss: 0.7370 - val_accuracy: 0.7754\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5370 - accuracy: 0.8275 - val_loss: 0.9720 - val_accuracy: 0.7018\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5254 - accuracy: 0.8324 - val_loss: 0.8212 - val_accuracy: 0.7484\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.5066 - accuracy: 0.8353 - val_loss: 0.7336 - val_accuracy: 0.7757\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4961 - accuracy: 0.8400 - val_loss: 0.7208 - val_accuracy: 0.7822\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4845 - accuracy: 0.8432 - val_loss: 0.7788 - val_accuracy: 0.7639\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4815 - accuracy: 0.8441 - val_loss: 0.8651 - val_accuracy: 0.7397\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.4824 - accuracy: 0.8442 - val_loss: 0.7527 - val_accuracy: 0.7816\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4682 - accuracy: 0.8470 - val_loss: 0.6441 - val_accuracy: 0.8077\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4629 - accuracy: 0.8482 - val_loss: 1.0707 - val_accuracy: 0.6891\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4487 - accuracy: 0.8545 - val_loss: 0.7170 - val_accuracy: 0.7896\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4496 - accuracy: 0.8554 - val_loss: 0.8063 - val_accuracy: 0.7596\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4389 - accuracy: 0.8562 - val_loss: 0.8075 - val_accuracy: 0.7639\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4364 - accuracy: 0.8573 - val_loss: 0.8090 - val_accuracy: 0.7532\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4297 - accuracy: 0.8622 - val_loss: 0.7122 - val_accuracy: 0.7817\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4209 - accuracy: 0.8607 - val_loss: 0.6899 - val_accuracy: 0.7964\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4200 - accuracy: 0.8640 - val_loss: 0.6597 - val_accuracy: 0.8087\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4168 - accuracy: 0.8640 - val_loss: 0.7546 - val_accuracy: 0.7720\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4041 - accuracy: 0.8682 - val_loss: 0.7766 - val_accuracy: 0.7798\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4127 - accuracy: 0.8649 - val_loss: 0.6764 - val_accuracy: 0.7966\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4026 - accuracy: 0.8679 - val_loss: 0.6699 - val_accuracy: 0.8069\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3846 - accuracy: 0.8749 - val_loss: 0.6397 - val_accuracy: 0.8204\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3919 - accuracy: 0.8710 - val_loss: 0.7943 - val_accuracy: 0.7701\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3890 - accuracy: 0.8716 - val_loss: 0.8234 - val_accuracy: 0.7649\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3875 - accuracy: 0.8729 - val_loss: 0.6634 - val_accuracy: 0.8082\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3816 - accuracy: 0.8735 - val_loss: 0.7108 - val_accuracy: 0.7941\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3707 - accuracy: 0.8784 - val_loss: 0.7521 - val_accuracy: 0.7812\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3718 - accuracy: 0.8771 - val_loss: 0.7642 - val_accuracy: 0.7759\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3638 - accuracy: 0.8801 - val_loss: 0.6561 - val_accuracy: 0.8130\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3598 - accuracy: 0.8819 - val_loss: 0.6587 - val_accuracy: 0.8108\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3544 - accuracy: 0.8822 - val_loss: 0.7101 - val_accuracy: 0.8010\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3639 - accuracy: 0.8802 - val_loss: 0.7520 - val_accuracy: 0.7926\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3617 - accuracy: 0.8798 - val_loss: 0.6977 - val_accuracy: 0.8008\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3559 - accuracy: 0.8817 - val_loss: 0.6902 - val_accuracy: 0.8022\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3576 - accuracy: 0.8825 - val_loss: 0.8486 - val_accuracy: 0.7502\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3426 - accuracy: 0.8856 - val_loss: 0.7565 - val_accuracy: 0.7873\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3511 - accuracy: 0.8841 - val_loss: 0.6852 - val_accuracy: 0.8062\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3402 - accuracy: 0.8882 - val_loss: 0.7116 - val_accuracy: 0.7908\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3405 - accuracy: 0.8873 - val_loss: 0.6317 - val_accuracy: 0.8236\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3346 - accuracy: 0.8886 - val_loss: 0.7398 - val_accuracy: 0.7941\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3311 - accuracy: 0.8911 - val_loss: 0.6360 - val_accuracy: 0.8286\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3260 - accuracy: 0.8920 - val_loss: 0.6499 - val_accuracy: 0.8233\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3308 - accuracy: 0.8896 - val_loss: 0.6941 - val_accuracy: 0.8102\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3274 - accuracy: 0.8908 - val_loss: 0.6111 - val_accuracy: 0.8349\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3249 - accuracy: 0.8902 - val_loss: 0.8964 - val_accuracy: 0.7463\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3264 - accuracy: 0.8901 - val_loss: 0.7436 - val_accuracy: 0.7881\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3176 - accuracy: 0.8936 - val_loss: 0.6205 - val_accuracy: 0.8327\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3295 - accuracy: 0.8907 - val_loss: 0.7110 - val_accuracy: 0.7993\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3246 - accuracy: 0.8916 - val_loss: 0.6402 - val_accuracy: 0.8307\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3213 - accuracy: 0.8928 - val_loss: 0.7482 - val_accuracy: 0.7931\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3116 - accuracy: 0.8960 - val_loss: 0.5855 - val_accuracy: 0.8474\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3176 - accuracy: 0.8930 - val_loss: 0.8223 - val_accuracy: 0.7889\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3115 - accuracy: 0.8955 - val_loss: 0.7160 - val_accuracy: 0.8089\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3133 - accuracy: 0.8951 - val_loss: 0.6947 - val_accuracy: 0.8093\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3035 - accuracy: 0.8990 - val_loss: 0.5974 - val_accuracy: 0.8438\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3036 - accuracy: 0.8985 - val_loss: 0.6965 - val_accuracy: 0.8126\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3036 - accuracy: 0.8991 - val_loss: 0.7660 - val_accuracy: 0.7827\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3008 - accuracy: 0.8993 - val_loss: 0.7244 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2996 - accuracy: 0.8995 - val_loss: 0.6551 - val_accuracy: 0.8306\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2987 - accuracy: 0.9004 - val_loss: 0.7537 - val_accuracy: 0.8002\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2979 - accuracy: 0.8988 - val_loss: 0.6285 - val_accuracy: 0.8261\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2917 - accuracy: 0.9004 - val_loss: 0.7411 - val_accuracy: 0.8076\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2936 - accuracy: 0.8997 - val_loss: 0.6349 - val_accuracy: 0.8334\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3012 - accuracy: 0.8994 - val_loss: 0.6575 - val_accuracy: 0.8247\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2918 - accuracy: 0.9021 - val_loss: 0.7874 - val_accuracy: 0.7947\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3040 - accuracy: 0.8979 - val_loss: 0.7715 - val_accuracy: 0.8048\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2847 - accuracy: 0.9035 - val_loss: 0.6929 - val_accuracy: 0.8203\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2873 - accuracy: 0.9037 - val_loss: 0.7028 - val_accuracy: 0.8140\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2822 - accuracy: 0.9049 - val_loss: 0.6688 - val_accuracy: 0.8227\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2896 - accuracy: 0.9033 - val_loss: 0.6790 - val_accuracy: 0.8176\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2912 - accuracy: 0.9030 - val_loss: 0.6867 - val_accuracy: 0.8159\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2879 - accuracy: 0.9041 - val_loss: 0.7109 - val_accuracy: 0.8179\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2842 - accuracy: 0.9051 - val_loss: 0.7402 - val_accuracy: 0.8011\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2835 - accuracy: 0.9059 - val_loss: 0.6724 - val_accuracy: 0.8216\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2814 - accuracy: 0.9056 - val_loss: 0.6751 - val_accuracy: 0.8220\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2792 - accuracy: 0.9072 - val_loss: 0.6960 - val_accuracy: 0.8196\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2754 - accuracy: 0.9068 - val_loss: 0.7964 - val_accuracy: 0.7985\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2723 - accuracy: 0.9081 - val_loss: 0.6809 - val_accuracy: 0.8215\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2838 - accuracy: 0.9049 - val_loss: 0.6802 - val_accuracy: 0.8274\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2739 - accuracy: 0.9080 - val_loss: 0.6628 - val_accuracy: 0.8244\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2690 - accuracy: 0.9096 - val_loss: 0.6433 - val_accuracy: 0.8364\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2724 - accuracy: 0.9092 - val_loss: 0.6393 - val_accuracy: 0.8351\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2687 - accuracy: 0.9121 - val_loss: 0.6070 - val_accuracy: 0.8377\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6070 - accuracy: 0.8377\n",
      "Try 1/100: Best_val_acc: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9d7f2d9668>, lr: 0.022564673140104987\n",
      "\n",
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 69ms/step - loss: 1.7253 - accuracy: 0.4121 - val_loss: 2.1374 - val_accuracy: 0.3428\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 1.0782 - accuracy: 0.6592 - val_loss: 1.2654 - val_accuracy: 0.5882\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.9486 - accuracy: 0.7027 - val_loss: 1.1359 - val_accuracy: 0.6330\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.8574 - accuracy: 0.7311 - val_loss: 1.3379 - val_accuracy: 0.5731\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.7800 - accuracy: 0.7569 - val_loss: 2.0907 - val_accuracy: 0.4143\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.7208 - accuracy: 0.7746 - val_loss: 1.1645 - val_accuracy: 0.6313\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6821 - accuracy: 0.7859 - val_loss: 0.9048 - val_accuracy: 0.7174\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6733 - accuracy: 0.7878 - val_loss: 1.0164 - val_accuracy: 0.6731\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6281 - accuracy: 0.8028 - val_loss: 1.0525 - val_accuracy: 0.6719\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6020 - accuracy: 0.8107 - val_loss: 1.2006 - val_accuracy: 0.6210\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5877 - accuracy: 0.8144 - val_loss: 0.8413 - val_accuracy: 0.7389\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5634 - accuracy: 0.8225 - val_loss: 1.0911 - val_accuracy: 0.6632\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5458 - accuracy: 0.8275 - val_loss: 1.1648 - val_accuracy: 0.6617\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5400 - accuracy: 0.8274 - val_loss: 0.8925 - val_accuracy: 0.7157\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5211 - accuracy: 0.8350 - val_loss: 0.9241 - val_accuracy: 0.7138\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5045 - accuracy: 0.8382 - val_loss: 0.9749 - val_accuracy: 0.6978\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4889 - accuracy: 0.8452 - val_loss: 0.9894 - val_accuracy: 0.7054\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4992 - accuracy: 0.8411 - val_loss: 0.9069 - val_accuracy: 0.7229\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4725 - accuracy: 0.8504 - val_loss: 0.8802 - val_accuracy: 0.7321\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4656 - accuracy: 0.8507 - val_loss: 0.7636 - val_accuracy: 0.7722\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4612 - accuracy: 0.8520 - val_loss: 0.6634 - val_accuracy: 0.7941\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4438 - accuracy: 0.8564 - val_loss: 0.6665 - val_accuracy: 0.7982\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4354 - accuracy: 0.8611 - val_loss: 0.6213 - val_accuracy: 0.8102\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4276 - accuracy: 0.8618 - val_loss: 0.6602 - val_accuracy: 0.8029\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4134 - accuracy: 0.8665 - val_loss: 0.6784 - val_accuracy: 0.7998\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4157 - accuracy: 0.8658 - val_loss: 0.7039 - val_accuracy: 0.7901\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4072 - accuracy: 0.8675 - val_loss: 0.7904 - val_accuracy: 0.7701\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4043 - accuracy: 0.8682 - val_loss: 0.5689 - val_accuracy: 0.8243\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3881 - accuracy: 0.8748 - val_loss: 0.7804 - val_accuracy: 0.7697\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3864 - accuracy: 0.8754 - val_loss: 0.8824 - val_accuracy: 0.7480\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3923 - accuracy: 0.8725 - val_loss: 0.5925 - val_accuracy: 0.8253\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3767 - accuracy: 0.8780 - val_loss: 0.6655 - val_accuracy: 0.8011\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.3784 - accuracy: 0.8772 - val_loss: 0.7016 - val_accuracy: 0.7912\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 68ms/step - loss: 0.3663 - accuracy: 0.8789 - val_loss: 0.6459 - val_accuracy: 0.8090\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.3697 - accuracy: 0.8802 - val_loss: 0.6491 - val_accuracy: 0.8071\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3583 - accuracy: 0.8830 - val_loss: 0.6072 - val_accuracy: 0.8221\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3434 - accuracy: 0.8870 - val_loss: 0.6212 - val_accuracy: 0.8163\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3496 - accuracy: 0.8866 - val_loss: 0.6171 - val_accuracy: 0.8237\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3457 - accuracy: 0.8876 - val_loss: 0.6595 - val_accuracy: 0.8081\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3353 - accuracy: 0.8895 - val_loss: 0.6741 - val_accuracy: 0.8009\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3365 - accuracy: 0.8892 - val_loss: 0.5532 - val_accuracy: 0.8401\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3336 - accuracy: 0.8907 - val_loss: 0.8488 - val_accuracy: 0.7622\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3350 - accuracy: 0.8880 - val_loss: 0.6322 - val_accuracy: 0.8126\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3236 - accuracy: 0.8929 - val_loss: 0.6760 - val_accuracy: 0.8068\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3227 - accuracy: 0.8932 - val_loss: 0.7259 - val_accuracy: 0.7834\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3244 - accuracy: 0.8929 - val_loss: 0.6098 - val_accuracy: 0.8266\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3197 - accuracy: 0.8936 - val_loss: 0.6962 - val_accuracy: 0.8047\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3159 - accuracy: 0.8952 - val_loss: 0.6254 - val_accuracy: 0.8176\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3100 - accuracy: 0.8967 - val_loss: 0.6362 - val_accuracy: 0.8215\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3062 - accuracy: 0.8988 - val_loss: 0.6850 - val_accuracy: 0.8027\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3116 - accuracy: 0.8971 - val_loss: 0.6791 - val_accuracy: 0.8099\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3034 - accuracy: 0.9012 - val_loss: 0.6112 - val_accuracy: 0.8277\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2936 - accuracy: 0.9012 - val_loss: 0.7189 - val_accuracy: 0.7880\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2975 - accuracy: 0.9006 - val_loss: 0.6171 - val_accuracy: 0.8238\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2904 - accuracy: 0.9040 - val_loss: 0.6146 - val_accuracy: 0.8279\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2962 - accuracy: 0.9011 - val_loss: 0.6366 - val_accuracy: 0.8190\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2867 - accuracy: 0.9033 - val_loss: 0.5813 - val_accuracy: 0.8356\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2858 - accuracy: 0.9046 - val_loss: 0.6311 - val_accuracy: 0.8211\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2917 - accuracy: 0.9031 - val_loss: 0.6678 - val_accuracy: 0.8138\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.2776 - accuracy: 0.9067 - val_loss: 0.5347 - val_accuracy: 0.8542\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2793 - accuracy: 0.9067 - val_loss: 0.5949 - val_accuracy: 0.8304\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2674 - accuracy: 0.9109 - val_loss: 0.6188 - val_accuracy: 0.8310\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2749 - accuracy: 0.9088 - val_loss: 0.7485 - val_accuracy: 0.7999\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2721 - accuracy: 0.9101 - val_loss: 0.6620 - val_accuracy: 0.8129\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2773 - accuracy: 0.9068 - val_loss: 0.6477 - val_accuracy: 0.8222\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2687 - accuracy: 0.9103 - val_loss: 0.5793 - val_accuracy: 0.8326\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2722 - accuracy: 0.9087 - val_loss: 0.6486 - val_accuracy: 0.8201\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2628 - accuracy: 0.9117 - val_loss: 0.7172 - val_accuracy: 0.8067\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2679 - accuracy: 0.9096 - val_loss: 0.6890 - val_accuracy: 0.8108\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2658 - accuracy: 0.9105 - val_loss: 0.6623 - val_accuracy: 0.8135\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2726 - accuracy: 0.9088 - val_loss: 0.7393 - val_accuracy: 0.7921\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2594 - accuracy: 0.9135 - val_loss: 0.5723 - val_accuracy: 0.8449\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2569 - accuracy: 0.9141 - val_loss: 0.5409 - val_accuracy: 0.8479\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2626 - accuracy: 0.9119 - val_loss: 0.6670 - val_accuracy: 0.8211\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2522 - accuracy: 0.9169 - val_loss: 0.6020 - val_accuracy: 0.8293\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2514 - accuracy: 0.9165 - val_loss: 0.5312 - val_accuracy: 0.8578\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2496 - accuracy: 0.9152 - val_loss: 0.5884 - val_accuracy: 0.8366\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2510 - accuracy: 0.9146 - val_loss: 0.5335 - val_accuracy: 0.8551\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2456 - accuracy: 0.9179 - val_loss: 0.5571 - val_accuracy: 0.8468\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2469 - accuracy: 0.9169 - val_loss: 0.5762 - val_accuracy: 0.8476\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2561 - accuracy: 0.9139 - val_loss: 0.5894 - val_accuracy: 0.8381\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2458 - accuracy: 0.9175 - val_loss: 0.6634 - val_accuracy: 0.8276\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2402 - accuracy: 0.9203 - val_loss: 0.5911 - val_accuracy: 0.8388\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2337 - accuracy: 0.9219 - val_loss: 0.5786 - val_accuracy: 0.8484\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2392 - accuracy: 0.9203 - val_loss: 0.6152 - val_accuracy: 0.8378\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2439 - accuracy: 0.9174 - val_loss: 0.8483 - val_accuracy: 0.7707\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2409 - accuracy: 0.9189 - val_loss: 0.5915 - val_accuracy: 0.8425\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2360 - accuracy: 0.9220 - val_loss: 0.5564 - val_accuracy: 0.8502\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2295 - accuracy: 0.9225 - val_loss: 0.6249 - val_accuracy: 0.8359\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2357 - accuracy: 0.9205 - val_loss: 0.7049 - val_accuracy: 0.8199\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2384 - accuracy: 0.9187 - val_loss: 0.5945 - val_accuracy: 0.8429\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2352 - accuracy: 0.9203 - val_loss: 0.6436 - val_accuracy: 0.8294\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2247 - accuracy: 0.9238 - val_loss: 0.6196 - val_accuracy: 0.8362\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2365 - accuracy: 0.9205 - val_loss: 0.7090 - val_accuracy: 0.8169\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2461 - accuracy: 0.9171 - val_loss: 0.7309 - val_accuracy: 0.8113\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2392 - accuracy: 0.9187 - val_loss: 0.7161 - val_accuracy: 0.8099\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2354 - accuracy: 0.9206 - val_loss: 0.5820 - val_accuracy: 0.8442\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2292 - accuracy: 0.9224 - val_loss: 0.6354 - val_accuracy: 0.8312\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.2337 - accuracy: 0.9211 - val_loss: 0.7294 - val_accuracy: 0.8038\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2352 - accuracy: 0.9205 - val_loss: 0.6396 - val_accuracy: 0.8274\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.8274\n",
      "Try 2/100: Best_val_acc: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9d7541f390>, lr: 0.004816346097344951\n",
      "\n",
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 71ms/step - loss: 2.1928 - accuracy: 0.2376 - val_loss: 2.2194 - val_accuracy: 0.2757\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 68ms/step - loss: 1.4840 - accuracy: 0.5253 - val_loss: 1.9564 - val_accuracy: 0.5513\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 1.1572 - accuracy: 0.6457 - val_loss: 1.7221 - val_accuracy: 0.6164\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 1.0141 - accuracy: 0.6909 - val_loss: 1.4805 - val_accuracy: 0.6653\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.9296 - accuracy: 0.7157 - val_loss: 1.3421 - val_accuracy: 0.6694\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.8602 - accuracy: 0.7389 - val_loss: 1.1320 - val_accuracy: 0.7070\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.8078 - accuracy: 0.7546 - val_loss: 0.9887 - val_accuracy: 0.7328\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.7698 - accuracy: 0.7644 - val_loss: 0.9678 - val_accuracy: 0.7176\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.7358 - accuracy: 0.7761 - val_loss: 0.8332 - val_accuracy: 0.7678\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.7032 - accuracy: 0.7846 - val_loss: 0.7988 - val_accuracy: 0.7678\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6758 - accuracy: 0.7938 - val_loss: 0.8054 - val_accuracy: 0.7533\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6560 - accuracy: 0.7965 - val_loss: 0.8257 - val_accuracy: 0.7398\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6380 - accuracy: 0.8022 - val_loss: 0.7035 - val_accuracy: 0.7896\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6165 - accuracy: 0.8099 - val_loss: 0.6847 - val_accuracy: 0.7933\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6029 - accuracy: 0.8141 - val_loss: 0.7064 - val_accuracy: 0.7863\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5825 - accuracy: 0.8193 - val_loss: 0.6713 - val_accuracy: 0.7977\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5693 - accuracy: 0.8236 - val_loss: 0.6736 - val_accuracy: 0.7943\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5608 - accuracy: 0.8263 - val_loss: 0.6575 - val_accuracy: 0.7977\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5476 - accuracy: 0.8301 - val_loss: 0.6637 - val_accuracy: 0.7972\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5377 - accuracy: 0.8330 - val_loss: 0.6007 - val_accuracy: 0.8216\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5273 - accuracy: 0.8372 - val_loss: 0.6310 - val_accuracy: 0.8115\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5144 - accuracy: 0.8408 - val_loss: 0.6245 - val_accuracy: 0.8089\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5087 - accuracy: 0.8412 - val_loss: 0.6683 - val_accuracy: 0.8009\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5011 - accuracy: 0.8442 - val_loss: 0.6133 - val_accuracy: 0.8193\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4930 - accuracy: 0.8461 - val_loss: 0.5787 - val_accuracy: 0.8262\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4848 - accuracy: 0.8500 - val_loss: 0.6769 - val_accuracy: 0.7956\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4845 - accuracy: 0.8486 - val_loss: 0.5822 - val_accuracy: 0.8233\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4783 - accuracy: 0.8495 - val_loss: 0.6533 - val_accuracy: 0.7973\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4703 - accuracy: 0.8535 - val_loss: 0.6267 - val_accuracy: 0.8089\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4636 - accuracy: 0.8545 - val_loss: 0.5666 - val_accuracy: 0.8290\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4601 - accuracy: 0.8555 - val_loss: 0.5633 - val_accuracy: 0.8303\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4530 - accuracy: 0.8586 - val_loss: 0.5732 - val_accuracy: 0.8254\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4408 - accuracy: 0.8620 - val_loss: 0.5585 - val_accuracy: 0.8306\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4406 - accuracy: 0.8611 - val_loss: 0.5427 - val_accuracy: 0.8392\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4293 - accuracy: 0.8650 - val_loss: 0.5683 - val_accuracy: 0.8258\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4291 - accuracy: 0.8663 - val_loss: 0.6098 - val_accuracy: 0.8116\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4245 - accuracy: 0.8668 - val_loss: 0.5371 - val_accuracy: 0.8395\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4194 - accuracy: 0.8680 - val_loss: 0.5819 - val_accuracy: 0.8269\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4151 - accuracy: 0.8691 - val_loss: 0.5908 - val_accuracy: 0.8192\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4158 - accuracy: 0.8691 - val_loss: 0.5817 - val_accuracy: 0.8211\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4076 - accuracy: 0.8731 - val_loss: 0.5589 - val_accuracy: 0.8326\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.4028 - accuracy: 0.8732 - val_loss: 0.6579 - val_accuracy: 0.8002\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.4020 - accuracy: 0.8740 - val_loss: 0.5662 - val_accuracy: 0.8283\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 67ms/step - loss: 0.3965 - accuracy: 0.8736 - val_loss: 0.5214 - val_accuracy: 0.8468\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3928 - accuracy: 0.8773 - val_loss: 0.5344 - val_accuracy: 0.8411\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3933 - accuracy: 0.8752 - val_loss: 0.6321 - val_accuracy: 0.8066\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3934 - accuracy: 0.8745 - val_loss: 0.5405 - val_accuracy: 0.8383\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3808 - accuracy: 0.8778 - val_loss: 0.6113 - val_accuracy: 0.8120\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3825 - accuracy: 0.8787 - val_loss: 0.5718 - val_accuracy: 0.8288\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3729 - accuracy: 0.8810 - val_loss: 0.5786 - val_accuracy: 0.8248\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3758 - accuracy: 0.8812 - val_loss: 0.6197 - val_accuracy: 0.8141\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3722 - accuracy: 0.8819 - val_loss: 0.6124 - val_accuracy: 0.8123\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3651 - accuracy: 0.8835 - val_loss: 0.5527 - val_accuracy: 0.8347\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3634 - accuracy: 0.8842 - val_loss: 0.5377 - val_accuracy: 0.8420\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3558 - accuracy: 0.8868 - val_loss: 0.5064 - val_accuracy: 0.8492\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3531 - accuracy: 0.8875 - val_loss: 0.5506 - val_accuracy: 0.8339\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3584 - accuracy: 0.8865 - val_loss: 0.5106 - val_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3479 - accuracy: 0.8901 - val_loss: 0.5419 - val_accuracy: 0.8389\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3546 - accuracy: 0.8872 - val_loss: 0.5388 - val_accuracy: 0.8418\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3446 - accuracy: 0.8908 - val_loss: 0.5344 - val_accuracy: 0.8399\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3438 - accuracy: 0.8892 - val_loss: 0.5256 - val_accuracy: 0.8424\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3401 - accuracy: 0.8906 - val_loss: 0.5121 - val_accuracy: 0.8464\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3427 - accuracy: 0.8905 - val_loss: 0.6044 - val_accuracy: 0.8214\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3376 - accuracy: 0.8910 - val_loss: 0.5695 - val_accuracy: 0.8305\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3387 - accuracy: 0.8918 - val_loss: 0.5605 - val_accuracy: 0.8341\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3339 - accuracy: 0.8912 - val_loss: 0.5319 - val_accuracy: 0.8431\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3293 - accuracy: 0.8950 - val_loss: 0.5507 - val_accuracy: 0.8334\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3247 - accuracy: 0.8944 - val_loss: 0.5508 - val_accuracy: 0.8366\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3229 - accuracy: 0.8959 - val_loss: 0.5091 - val_accuracy: 0.8496\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3223 - accuracy: 0.8977 - val_loss: 0.5681 - val_accuracy: 0.8323\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3182 - accuracy: 0.8980 - val_loss: 0.5522 - val_accuracy: 0.8364\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3177 - accuracy: 0.8987 - val_loss: 0.5450 - val_accuracy: 0.8402\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3118 - accuracy: 0.8993 - val_loss: 0.5095 - val_accuracy: 0.8512\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3072 - accuracy: 0.9001 - val_loss: 0.5315 - val_accuracy: 0.8424\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3067 - accuracy: 0.9009 - val_loss: 0.5267 - val_accuracy: 0.8464\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3074 - accuracy: 0.9022 - val_loss: 0.5650 - val_accuracy: 0.8363\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3048 - accuracy: 0.9021 - val_loss: 0.5219 - val_accuracy: 0.8491\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3038 - accuracy: 0.9026 - val_loss: 0.5180 - val_accuracy: 0.8492\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2998 - accuracy: 0.9020 - val_loss: 0.5711 - val_accuracy: 0.8322\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3005 - accuracy: 0.9028 - val_loss: 0.5177 - val_accuracy: 0.8463\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2918 - accuracy: 0.9051 - val_loss: 0.5123 - val_accuracy: 0.8518\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2948 - accuracy: 0.9050 - val_loss: 0.5143 - val_accuracy: 0.8459\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2907 - accuracy: 0.9060 - val_loss: 0.6034 - val_accuracy: 0.8256\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2919 - accuracy: 0.9061 - val_loss: 0.4993 - val_accuracy: 0.8573\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2824 - accuracy: 0.9080 - val_loss: 0.5550 - val_accuracy: 0.8389\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2855 - accuracy: 0.9071 - val_loss: 0.5278 - val_accuracy: 0.8422\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2850 - accuracy: 0.9084 - val_loss: 0.5384 - val_accuracy: 0.8422\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2848 - accuracy: 0.9070 - val_loss: 0.5616 - val_accuracy: 0.8371\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2782 - accuracy: 0.9092 - val_loss: 0.5146 - val_accuracy: 0.8527\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2751 - accuracy: 0.9099 - val_loss: 0.5093 - val_accuracy: 0.8545\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2761 - accuracy: 0.9106 - val_loss: 0.5397 - val_accuracy: 0.8439\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2743 - accuracy: 0.9108 - val_loss: 0.5423 - val_accuracy: 0.8454\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2710 - accuracy: 0.9126 - val_loss: 0.5597 - val_accuracy: 0.8348\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2721 - accuracy: 0.9110 - val_loss: 0.4954 - val_accuracy: 0.8630\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2701 - accuracy: 0.9105 - val_loss: 0.5303 - val_accuracy: 0.8488\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2641 - accuracy: 0.9137 - val_loss: 0.5413 - val_accuracy: 0.8415\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2658 - accuracy: 0.9143 - val_loss: 0.5185 - val_accuracy: 0.8523\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2631 - accuracy: 0.9139 - val_loss: 0.5377 - val_accuracy: 0.8489\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.2620 - accuracy: 0.9147 - val_loss: 0.5401 - val_accuracy: 0.8468\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.2655 - accuracy: 0.9138 - val_loss: 0.4992 - val_accuracy: 0.8563\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4992 - accuracy: 0.8563\n",
      "Try 3/100: Best_val_acc: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9d7537da20>, lr: 0.0002977345377574985\n",
      "\n",
      "Epoch 1/100\n",
      "42/42 [==============================] - 3s 71ms/step - loss: 2.5598 - accuracy: 0.1448 - val_loss: 9.9651 - val_accuracy: 0.1269\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 1.8044 - accuracy: 0.3378 - val_loss: 3.5195 - val_accuracy: 0.2154\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 1.4691 - accuracy: 0.4813 - val_loss: 1.7601 - val_accuracy: 0.4506\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 1.2066 - accuracy: 0.6021 - val_loss: 1.8496 - val_accuracy: 0.4787\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 1.0469 - accuracy: 0.6635 - val_loss: 2.0875 - val_accuracy: 0.4409\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.9429 - accuracy: 0.6988 - val_loss: 2.0182 - val_accuracy: 0.4398\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.8711 - accuracy: 0.7272 - val_loss: 1.2096 - val_accuracy: 0.6115\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.8218 - accuracy: 0.7400 - val_loss: 1.3241 - val_accuracy: 0.5826\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.7903 - accuracy: 0.7503 - val_loss: 1.1253 - val_accuracy: 0.6392\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.7518 - accuracy: 0.7622 - val_loss: 1.2508 - val_accuracy: 0.6329\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.6995 - accuracy: 0.7794 - val_loss: 1.1036 - val_accuracy: 0.6521\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.6835 - accuracy: 0.7823 - val_loss: 0.8239 - val_accuracy: 0.7455\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.6622 - accuracy: 0.7899 - val_loss: 1.1147 - val_accuracy: 0.6682\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.6553 - accuracy: 0.7915 - val_loss: 0.7938 - val_accuracy: 0.7612\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.6196 - accuracy: 0.8022 - val_loss: 0.8677 - val_accuracy: 0.7479\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5979 - accuracy: 0.8091 - val_loss: 0.9848 - val_accuracy: 0.7168\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5922 - accuracy: 0.8123 - val_loss: 1.1134 - val_accuracy: 0.6599\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.5901 - accuracy: 0.8122 - val_loss: 0.7358 - val_accuracy: 0.7761\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5788 - accuracy: 0.8160 - val_loss: 0.7319 - val_accuracy: 0.7723\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5608 - accuracy: 0.8200 - val_loss: 0.7414 - val_accuracy: 0.7724\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5437 - accuracy: 0.8241 - val_loss: 0.7146 - val_accuracy: 0.7791\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5445 - accuracy: 0.8267 - val_loss: 0.9917 - val_accuracy: 0.6939\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5467 - accuracy: 0.8244 - val_loss: 0.6377 - val_accuracy: 0.8104\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5157 - accuracy: 0.8339 - val_loss: 0.6357 - val_accuracy: 0.8106\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5179 - accuracy: 0.8327 - val_loss: 0.7159 - val_accuracy: 0.7844\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.5080 - accuracy: 0.8377 - val_loss: 0.7789 - val_accuracy: 0.7649\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.5074 - accuracy: 0.8355 - val_loss: 0.6751 - val_accuracy: 0.7958\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4935 - accuracy: 0.8422 - val_loss: 0.6733 - val_accuracy: 0.7988\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4938 - accuracy: 0.8410 - val_loss: 0.7318 - val_accuracy: 0.7804\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4887 - accuracy: 0.8429 - val_loss: 0.6619 - val_accuracy: 0.7972\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4744 - accuracy: 0.8471 - val_loss: 0.7632 - val_accuracy: 0.7647\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4776 - accuracy: 0.8465 - val_loss: 0.6573 - val_accuracy: 0.8056\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4614 - accuracy: 0.8509 - val_loss: 0.7113 - val_accuracy: 0.7868\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4626 - accuracy: 0.8495 - val_loss: 0.6299 - val_accuracy: 0.8077\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4565 - accuracy: 0.8526 - val_loss: 0.7763 - val_accuracy: 0.7627\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4486 - accuracy: 0.8568 - val_loss: 0.7130 - val_accuracy: 0.7844\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4440 - accuracy: 0.8574 - val_loss: 0.6860 - val_accuracy: 0.8039\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4454 - accuracy: 0.8547 - val_loss: 0.7497 - val_accuracy: 0.7812\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4392 - accuracy: 0.8581 - val_loss: 0.7543 - val_accuracy: 0.7800\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4365 - accuracy: 0.8595 - val_loss: 0.5833 - val_accuracy: 0.8315\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4353 - accuracy: 0.8589 - val_loss: 0.6381 - val_accuracy: 0.8102\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.4277 - accuracy: 0.8592 - val_loss: 0.6961 - val_accuracy: 0.7942\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4187 - accuracy: 0.8634 - val_loss: 0.6283 - val_accuracy: 0.8202\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4219 - accuracy: 0.8644 - val_loss: 0.6830 - val_accuracy: 0.8014\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4226 - accuracy: 0.8620 - val_loss: 0.8093 - val_accuracy: 0.7617\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4272 - accuracy: 0.8616 - val_loss: 0.7887 - val_accuracy: 0.7764\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4069 - accuracy: 0.8679 - val_loss: 0.7172 - val_accuracy: 0.7972\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4092 - accuracy: 0.8663 - val_loss: 0.6943 - val_accuracy: 0.7969\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.4059 - accuracy: 0.8681 - val_loss: 0.6699 - val_accuracy: 0.8011\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3997 - accuracy: 0.8693 - val_loss: 0.7186 - val_accuracy: 0.7872\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3912 - accuracy: 0.8718 - val_loss: 0.7716 - val_accuracy: 0.7811\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3895 - accuracy: 0.8720 - val_loss: 0.6190 - val_accuracy: 0.8226\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3948 - accuracy: 0.8705 - val_loss: 0.6434 - val_accuracy: 0.8118\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 3s 66ms/step - loss: 0.3848 - accuracy: 0.8741 - val_loss: 0.8062 - val_accuracy: 0.7722\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3852 - accuracy: 0.8752 - val_loss: 0.7187 - val_accuracy: 0.7939\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3816 - accuracy: 0.8745 - val_loss: 0.5977 - val_accuracy: 0.8319\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3729 - accuracy: 0.8783 - val_loss: 0.6429 - val_accuracy: 0.8171\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3767 - accuracy: 0.8759 - val_loss: 0.6847 - val_accuracy: 0.8071\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3718 - accuracy: 0.8783 - val_loss: 0.6268 - val_accuracy: 0.8198\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3732 - accuracy: 0.8775 - val_loss: 0.5932 - val_accuracy: 0.8250\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3706 - accuracy: 0.8778 - val_loss: 0.6976 - val_accuracy: 0.8024\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3659 - accuracy: 0.8808 - val_loss: 0.5833 - val_accuracy: 0.8334\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3677 - accuracy: 0.8785 - val_loss: 0.7131 - val_accuracy: 0.7989\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3646 - accuracy: 0.8802 - val_loss: 0.7611 - val_accuracy: 0.7911\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3708 - accuracy: 0.8786 - val_loss: 0.7269 - val_accuracy: 0.7946\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3580 - accuracy: 0.8825 - val_loss: 0.6588 - val_accuracy: 0.8127\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3604 - accuracy: 0.8818 - val_loss: 0.6212 - val_accuracy: 0.8299\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3518 - accuracy: 0.8835 - val_loss: 0.7015 - val_accuracy: 0.7984\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3527 - accuracy: 0.8839 - val_loss: 0.6461 - val_accuracy: 0.8199\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 3s 63ms/step - loss: 0.3583 - accuracy: 0.8815 - val_loss: 0.6683 - val_accuracy: 0.8102\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3456 - accuracy: 0.8866 - val_loss: 0.7052 - val_accuracy: 0.8002\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3502 - accuracy: 0.8834 - val_loss: 0.6199 - val_accuracy: 0.8272\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3515 - accuracy: 0.8844 - val_loss: 0.7198 - val_accuracy: 0.7957\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3525 - accuracy: 0.8852 - val_loss: 0.5908 - val_accuracy: 0.8344\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3493 - accuracy: 0.8850 - val_loss: 0.6670 - val_accuracy: 0.8163\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3368 - accuracy: 0.8870 - val_loss: 0.5799 - val_accuracy: 0.8367\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3356 - accuracy: 0.8886 - val_loss: 0.6673 - val_accuracy: 0.8212\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3454 - accuracy: 0.8857 - val_loss: 0.6979 - val_accuracy: 0.8044\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3417 - accuracy: 0.8871 - val_loss: 0.6673 - val_accuracy: 0.8154\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3371 - accuracy: 0.8879 - val_loss: 0.5940 - val_accuracy: 0.8347\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3341 - accuracy: 0.8894 - val_loss: 0.6558 - val_accuracy: 0.8239\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3327 - accuracy: 0.8918 - val_loss: 0.6986 - val_accuracy: 0.7989\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3452 - accuracy: 0.8858 - val_loss: 0.6812 - val_accuracy: 0.8126\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3307 - accuracy: 0.8908 - val_loss: 0.6347 - val_accuracy: 0.8168\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3335 - accuracy: 0.8911 - val_loss: 0.5530 - val_accuracy: 0.8476\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3292 - accuracy: 0.8909 - val_loss: 0.6307 - val_accuracy: 0.8261\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3162 - accuracy: 0.8959 - val_loss: 0.6205 - val_accuracy: 0.8279\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3213 - accuracy: 0.8934 - val_loss: 0.6698 - val_accuracy: 0.8165\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3298 - accuracy: 0.8916 - val_loss: 0.6782 - val_accuracy: 0.8138\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3269 - accuracy: 0.8930 - val_loss: 0.7255 - val_accuracy: 0.7981\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3257 - accuracy: 0.8919 - val_loss: 0.7802 - val_accuracy: 0.7887\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3261 - accuracy: 0.8927 - val_loss: 0.6753 - val_accuracy: 0.8142\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3281 - accuracy: 0.8916 - val_loss: 0.6245 - val_accuracy: 0.8271\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3200 - accuracy: 0.8956 - val_loss: 0.7154 - val_accuracy: 0.8022\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 3s 65ms/step - loss: 0.3144 - accuracy: 0.8946 - val_loss: 0.6459 - val_accuracy: 0.8246\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3276 - accuracy: 0.8925 - val_loss: 0.6862 - val_accuracy: 0.8047\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3080 - accuracy: 0.8982 - val_loss: 0.6730 - val_accuracy: 0.8171\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3052 - accuracy: 0.8985 - val_loss: 0.6581 - val_accuracy: 0.8266\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3082 - accuracy: 0.8982 - val_loss: 0.5387 - val_accuracy: 0.8546\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 3s 64ms/step - loss: 0.3118 - accuracy: 0.8973 - val_loss: 0.7416 - val_accuracy: 0.7932\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "summary of model None\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.7416 - accuracy: 0.7932\n",
      "Try 4/100: Best_val_acc: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9d7451f860>, lr: 0.034829545762290116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for k in range (1,5):\n",
    "  lr = math.pow(10, np.random.uniform(-5,-1))\n",
    "  best_accuracy = model_7(0.9, 0.99, lr)\n",
    "  print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}\\n\".format(k, 100, best_accuracy, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYaC5wvYAnC8"
   },
   "source": [
    "So for model_7 with learning rate = 0.0002977345377574985 we get test accuracy almost equal to 86% which is the highest till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuSrJhEbFiy6"
   },
   "outputs": [],
   "source": [
    "  model_7 = Sequential() \n",
    "  model_7.add(Dense(256, input_shape = (1024,)))\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Dropout(0.1))\n",
    "  \n",
    "  model_7.add(Dense(256))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(Dropout(0.1))\n",
    "\n",
    "  model_7.add(Dense(256))\n",
    "  model_7.add(BatchNormalization())\n",
    "  model_7.add(Activation('relu'))\n",
    "  model_7.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "  model_7.add(Dense(10))\n",
    "  model_7.add(Activation('softmax'))\n",
    "\n",
    "  ada_3 = optimizers.Adam(lr = 0.0002977345377574985)\n",
    "  model_7.compile(optimizer=ada_3, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "  #callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.01)\n",
    "  #model_7.fit(X_train, y_train, batch_size = 1000, epochs= 100, verbose = 1, validation_data=(X_test,y_test)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "gXmv4kULvEb8",
    "outputId": "400346ed-f6ec-4cb4-d08f-9f71753c6035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_74 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 399,626\n",
      "Trainable params: 398,090\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "A6eNZnVwGCXD",
    "outputId": "f8254368-3a42-4c42-f8dd-3af0cfd14f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5250 - accuracy: 0.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5249767899513245, 0.8500555753707886]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "MbEd-1QyfcfG"
   },
   "source": [
    " Since model_7 with Learing Rate = 0.0002977345377574985 is the best model so lets try to find what thsi model predicts for a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "MrIEJfJmDLsa",
    "outputId": "63c639f7-0a45-4dfb-b3bd-b1b89fd4afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities for 10th image [4.3330710e-06 2.4421807e-04 5.7140905e-09 2.4765029e-08 9.9974698e-01\n",
      " 3.8355341e-09 3.2894177e-06 1.1722869e-09 1.1573600e-06 1.8946908e-08]\n",
      "predicted image label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_test[10000].reshape(32,32), cmap = 'gray')\n",
    "y_pred = model_7.predict(X_test)\n",
    "print(\"probabilities for 10th image\", y_pred[10000])\n",
    "\n",
    "print(\"predicted image label\", np.argmax(y_pred[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRBrvKyWfbmD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "hR616kDRGLyf",
    "outputId": "a4cbf15d-32b7-4845-f1bd-60d14c410d01"
   },
   "source": [
    "# So we find that model_7 with Learning Rate = 0.0002977345377574985 is the best model for this dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "Xl2wQes1hyET",
    "outputId": "4531d259-bc75-49a2-fde2-967f38b54983"
   },
   "source": [
    "But our target is to also create a network which has least complexity as we increase the layers the no of weights and parameters increase and cost of creating the network also increases and a deeper network also consumes a lot of memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2XT3j4biYVR"
   },
   "source": [
    "# So we find that model_4 has much less parameters and less deeper than model_7 so complexity and memory usage is much less than Model_7. So model_4 can get more preference over model_7 in terms of industrial use though it has less accuracy than model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetworksProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
